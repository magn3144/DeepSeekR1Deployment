{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lQ4xC-yVVgqw"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/workspace/DeepSeekR1Deployment/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmagn3144\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Import basic libraries \n",
        "import logging\n",
        "import os\n",
        "import sys\n",
        "import re\n",
        "import math\n",
        "from dataclasses import dataclass, field\n",
        "from typing import List, Optional\n",
        "from dotenv import load_dotenv\n",
        "import numpy as np\n",
        "\n",
        "# Import PyTorch and Hugging Face Transformers\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    HfArgumentParser,\n",
        "    TrainingArguments,\n",
        "    set_seed,\n",
        "    TrainerCallback,\n",
        "    TrainerControl,\n",
        "    TrainerState,\n",
        ")\n",
        "from transformers.trainer_utils import get_last_checkpoint\n",
        "\n",
        "import wandb\n",
        "\n",
        "# Import dataset utilities\n",
        "import datasets\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Import libraries from TRL (Transformers Reinforcement Learning)\n",
        "from trl import (\n",
        "    AutoModelForCausalLMWithValueHead,\n",
        "    PPOConfig,\n",
        "    PPOTrainer,\n",
        "    GRPOTrainer,\n",
        "    GRPOConfig,\n",
        "    SFTTrainer\n",
        ")\n",
        "\n",
        "# Import math-related utilities\n",
        "from latex2sympy2_extended import NormalizationConfig\n",
        "from math_verify import LatexExtractionConfig, parse, verify\n",
        "\n",
        "load_dotenv()\n",
        "api_key = os.getenv(\"WANDB_API_KEY\")\n",
        "wandb.login(key=api_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nm1AHSltVgq4",
        "outputId": "46c12947-6912-49ef-c92f-617073ef013d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'system': \"Your role as an assistant involves thoroughly exploring questions through a systematic long thinking process before providing the final precise and accurate solutions. This requires engaging in a comprehensive cycle of analysis, summarizing, exploration, reassessment, reflection, backtracing, and iteration to develop well-considered thinking process. Please structure your response into two main sections: Thought and Solution. In the Thought section, detail your reasoning process using the specified format: <|begin_of_thought|> {thought with steps separated with '\\\\n\\\\n'} <|end_of_thought|> Each step should include detailed considerations such as analisying questions, summarizing relevant findings, brainstorming new ideas, verifying the accuracy of the current steps, refining any errors, and revisiting previous steps. In the Solution section, based on various attempts, explorations, and reflections from the Thought section, systematically present the final solution that you deem correct. The solution should remain a logical, accurate, concise expression style and detail necessary step needed to reach the conclusion, formatted as follows: <|begin_of_solution|> {final formatted, precise, and clear solution} <|end_of_solution|> Now, try to solve the following question through the above guidelines:\",\n",
              " 'conversations': [{'from': 'user',\n",
              "   'value': 'Return your final response within \\\\boxed{}. The operation $\\\\otimes$ is defined for all nonzero numbers by $a\\\\otimes b =\\\\frac{a^{2}}{b}$. Determine $[(1\\\\otimes 2)\\\\otimes 3]-[1\\\\otimes (2\\\\otimes 3)]$.\\n$\\\\text{(A)}\\\\ -\\\\frac{2}{3}\\\\qquad\\\\text{(B)}\\\\ -\\\\frac{1}{4}\\\\qquad\\\\text{(C)}\\\\ 0\\\\qquad\\\\text{(D)}\\\\ \\\\frac{1}{4}\\\\qquad\\\\text{(E)}\\\\ \\\\frac{2}{3}$'},\n",
              "  {'from': 'assistant',\n",
              "   'value': \"<|begin_of_thought|>\\n\\nOkay, let me try to figure out this problem. So, we have this operation defined as a‚äób = a¬≤/b. And we need to compute [(1‚äó2)‚äó3] - [1‚äó(2‚äó3)]. Then choose the correct answer from the options given. Alright, let's break it down step by step.\\n\\nFirst, I need to remember that the operation ‚äó is not associative, right? Because the problem is asking for the difference between two different groupings: (1‚äó2)‚äó3 and 1‚äó(2‚äó3). So, the order in which we perform the operations matters here. That's probably why there's a subtraction between them.\\n\\nLet me start by computing each part separately. Let's tackle the first part: (1‚äó2)‚äó3.\\n\\nStarting with the innermost operation, which is 1‚äó2. According to the definition, a‚äób = a¬≤/b. So here, a is 1 and b is 2. Plugging those in: 1¬≤ / 2 = 1/2. So, 1‚äó2 equals 1/2.\\n\\nNow, we take that result and perform the next operation with 3. So, (1‚äó2)‚äó3 becomes (1/2)‚äó3. Again, using the same definition: a is now 1/2 and b is 3. So, ( (1/2)¬≤ ) / 3 = (1/4) / 3 = 1/12. So, (1‚äó2)‚äó3 equals 1/12.\\n\\nAlright, that's the first part. Now let's compute the second part: 1‚äó(2‚äó3). Again, starting with the innermost operation, which is 2‚äó3. Applying the definition: a is 2 and b is 3. So, 2¬≤ / 3 = 4/3. Therefore, 2‚äó3 equals 4/3.\\n\\nNow, we need to compute 1‚äó(4/3). Here, a is 1 and b is 4/3. Using the operation definition: 1¬≤ / (4/3) = 1 / (4/3) = 3/4. So, 1‚äó(2‚äó3) equals 3/4.\\n\\nNow, the problem asks for the difference between the two results: [(1‚äó2)‚äó3] - [1‚äó(2‚äó3)] = (1/12) - (3/4). To subtract these fractions, they need a common denominator. The denominators are 12 and 4, so 12 is the common denominator.\\n\\nConverting 3/4 to twelfths: 3/4 = 9/12. So, 1/12 - 9/12 = (1 - 9)/12 = -8/12. Simplifying that fraction by dividing numerator and denominator by 4: -8/12 = -2/3.\\n\\nHmm, looking at the answer choices, option A is -2/3. So, is that the answer? Wait, but let me double-check my calculations to make sure I didn't make a mistake somewhere.\\n\\nFirst, checking (1‚äó2): 1¬≤ / 2 = 1/2. Correct. Then, (1/2)‚äó3: (1/2)¬≤ / 3 = (1/4)/3 = 1/12. That seems right.\\n\\nNow, for 2‚äó3: 2¬≤ / 3 = 4/3. Correct. Then, 1‚äó(4/3): 1¬≤ / (4/3) = 1 / (4/3) = 3/4. Yes, that's correct.\\n\\nSubtracting 3/4 from 1/12: 1/12 - 3/4. Convert 3/4 to 9/12, so 1/12 - 9/12 = -8/12 = -2/3. Yes, that all checks out. So the answer should be -2/3, which is option A.\\n\\nWait, but let me think again. The operation is defined for all nonzero numbers, so we don't have any issues with division by zero here. 2‚äó3 is 4/3, which is fine, and then 1‚äó(4/3) is 3/4. Correct.\\n\\nAlternatively, maybe there's a different way to approach the problem? Let me try expanding both expressions using variables to see if there's a pattern.\\n\\nLet‚Äôs denote the first expression: (a‚äób)‚äóc. Using the definition:\\n\\nFirst, compute a‚äób = a¬≤/b.\\n\\nThen, take that result and ‚äó with c: (a¬≤/b)‚äóc = ( (a¬≤/b)¬≤ ) / c = a‚Å¥ / (b¬≤ c).\\n\\nNow, the second expression: a‚äó(b‚äóc). First compute b‚äóc = b¬≤/c.\\n\\nThen, a‚äó(b¬≤/c) = a¬≤ / (b¬≤/c) = a¬≤ * (c / b¬≤) = (a¬≤ c) / b¬≤.\\n\\nTherefore, the difference between the two expressions is:\\n\\n(a‚Å¥ / (b¬≤ c)) - (a¬≤ c / b¬≤) = (a‚Å¥ - a¬≤ c¬≤) / (b¬≤ c) = a¬≤ (a¬≤ - c¬≤) / (b¬≤ c).\\n\\nHmm, factoring that, it's a¬≤ (a - c)(a + c) / (b¬≤ c).\\n\\nBut in our specific problem, a = 1, b = 2, c = 3. Plugging those values in:\\n\\n1¬≤ (1 - 3)(1 + 3) / (2¬≤ * 3) = 1 * (-2)(4) / (4 * 3) = (-8) / 12 = -2/3. Same result. So that confirms the answer is indeed -2/3.\\n\\nTherefore, I think my initial calculation was correct, and the answer is option A.\\n\\n**Final Answer**\\n\\\\boxed{A}\\n\\n<|end_of_thought|>\\n\\n<|begin_of_solution|>\\n\\nTo determine the value of \\\\([(1 \\\\otimes 2) \\\\otimes 3] - [1 \\\\otimes (2 \\\\otimes 3)]\\\\) where the operation \\\\(\\\\otimes\\\\) is defined by \\\\(a \\\\otimes b = \\\\frac{a^2}{b}\\\\), we proceed as follows:\\n\\nFirst, compute \\\\(1 \\\\otimes 2\\\\):\\n\\\\[\\n1 \\\\otimes 2 = \\\\frac{1^2}{2} = \\\\frac{1}{2}\\n\\\\]\\nNext, use this result to compute \\\\((1 \\\\otimes 2) \\\\otimes 3\\\\):\\n\\\\[\\n\\\\left(\\\\frac{1}{2}\\\\right) \\\\otimes 3 = \\\\frac{\\\\left(\\\\frac{1}{2}\\\\right)^2}{3} = \\\\frac{\\\\frac{1}{4}}{3} = \\\\frac{1}{12}\\n\\\\]\\n\\nNow, compute \\\\(2 \\\\otimes 3\\\\):\\n\\\\[\\n2 \\\\otimes 3 = \\\\frac{2^2}{3} = \\\\frac{4}{3}\\n\\\\]\\nThen, use this result to compute \\\\(1 \\\\otimes (2 \\\\otimes 3)\\\\):\\n\\\\[\\n1 \\\\otimes \\\\left(\\\\frac{4}{3}\\\\right) = \\\\frac{1^2}{\\\\frac{4}{3}} = \\\\frac{1}{\\\\frac{4}{3}} = \\\\frac{3}{4}\\n\\\\]\\n\\nFinally, find the difference between the two results:\\n\\\\[\\n\\\\frac{1}{12} - \\\\frac{3}{4} = \\\\frac{1}{12} - \\\\frac{9}{12} = \\\\frac{1 - 9}{12} = \\\\frac{-8}{12} = -\\\\frac{2}{3}\\n\\\\]\\n\\nThus, the answer is \\\\(\\\\boxed{A}\\\\).\\n\\n<|end_of_solution|>\"}]}"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load the \"AI-MO/NuminaMath-TIR\" dataset from DigitalLearningGmbH\n",
        "MATH_le = load_dataset(\"AI-MO/NuminaMath-TIR\", \"default\")\n",
        "\n",
        "# Access the first sample in the training set\n",
        "MATH_le['train'][0]\n",
        "\n",
        "# Load the \"Bespoke-Stratos-17k\" dataset from bespokelabs\n",
        "bespoke_rl = load_dataset(\"bespokelabs/Bespoke-Stratos-17k\", \"default\")\n",
        "\n",
        "# Access the first sample in the training set\n",
        "bespoke_rl['train'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "grmQEuCxVgrB",
        "outputId": "2ecf6108-f018-4c05-b823-e907f91ad067"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary size: 151665\n",
            "Model max length: 131072\n",
            "Pad token: <|endoftext|>\n",
            "EOS token: <|im_end|>\n",
            "Model parameters: 494,032,768\n"
          ]
        }
      ],
      "source": [
        "MODEL_NAME = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
        "OUTPUT_DIR = \"data/Qwen-GRPO-training\" # For saving our trained model\n",
        "\n",
        "# Create output directory if it doesn't exist\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Initialize tokenizer with chat template\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    trust_remote_code=True,\n",
        "    padding_side=\"right\"\n",
        ")\n",
        "\n",
        "# Set pad token if not set\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(f\"Vocabulary size: {len(tokenizer)}\")\n",
        "print(f\"Model max length: {tokenizer.model_max_length}\")\n",
        "print(f\"Pad token: {tokenizer.pad_token}\")\n",
        "print(f\"EOS token: {tokenizer.eos_token}\")\n",
        "\n",
        "# Initialize base model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "print(f\"Model parameters: {model.num_parameters():,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGWn0oYDVgrE",
        "outputId": "3e10074e-ab9a-45a4-a0bf-5e895e64ceaa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Input: how are you?\n",
            "Model Response: system\n",
            "You are Qwen, a helpful assistant.\n",
            "user\n",
            "how are you?\n",
            "assistant\n",
            "As an AI language model, I don't have feelings or emotions in the traditional sense. However, I'm always here to assist with any questions or tasks you may have! How can I help you today?\n"
          ]
        }
      ],
      "source": [
        "# Check CUDA availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Move model to the appropriate device\n",
        "model.to(device)\n",
        "\n",
        "# Test basic inference\n",
        "def test_model_inference(user_input: str):\n",
        "    \"\"\"Test basic model inference with the loaded model and tokenizer.\"\"\"\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are Qwen, a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": user_input}\n",
        "    ]\n",
        "\n",
        "    # Apply chat template\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "\n",
        "    # Tokenize and generate\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=100,\n",
        "        do_sample=True,\n",
        "        temperature=0.7\n",
        "    )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response\n",
        "\n",
        "# Test the model\n",
        "test_input = \"how are you?\"\n",
        "response = test_model_inference(test_input)\n",
        "print(f\"Test Input: {test_input}\")\n",
        "print(f\"Model Response: {response}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "DNsOH2URVgrG"
      },
      "outputs": [],
      "source": [
        "# DeepSeek system prompt for GRPO based training\n",
        "SYSTEM_PROMPT = (\n",
        "    \"A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant \"\n",
        "    \"first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning \"\n",
        "    \"process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., \"\n",
        "    \"<think> reasoning process here </think><answer> answer here </answer>\"\n",
        ")\n",
        "\n",
        "log_dict = {\n",
        "    \"train_loss\": [],\n",
        "    \"learning_rate\": [],\n",
        "    \"accuracy_reward\": [],\n",
        "    \"format_reward\": [],\n",
        "    \"reasoning_steps_reward\": [],\n",
        "    \"cosine_scaled_reward\": [],\n",
        "    \"repetition_penalty_reward\": [],\n",
        "    \"total_reward\": [],\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "O7pJISXpVgrH"
      },
      "outputs": [],
      "source": [
        "# Function to structure the training data\n",
        "def make_conversation(example):\n",
        "    \"\"\"Convert dataset examples into conversation format.\"\"\"\n",
        "    return {\n",
        "        \"prompt\": [\n",
        "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "            {\"role\": \"user\", \"content\": example[\"problem\"]},\n",
        "        ],\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "7GhKxEqqVgrI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train set size: 72441\n",
            "Test set size: 99\n"
          ]
        }
      ],
      "source": [
        "# Load and prepare dataset\n",
        "def load_math_dataset():\n",
        "    \"\"\"Load and prepare the mathematics dataset.\"\"\"\n",
        "    dataset = load_dataset(\n",
        "        \"AI-MO/NuminaMath-TIR\",\n",
        "        name=\"default\",\n",
        "        split=['train', 'test']\n",
        "    )\n",
        "\n",
        "    # Convert splits into dictionary\n",
        "    dataset = {\n",
        "        'train': dataset[0],\n",
        "        'test': dataset[1]\n",
        "    }\n",
        "\n",
        "    # Apply conversation format\n",
        "    for split in dataset:\n",
        "        dataset[split] = dataset[split].map(make_conversation)\n",
        "\n",
        "        # Remove 'messages' column if exists\n",
        "        if \"messages\" in dataset[split].column_names:\n",
        "            dataset[split] = dataset[split].remove_columns(\"messages\")\n",
        "\n",
        "    return dataset\n",
        "\n",
        "# Load our training dataset and printing train/test size\n",
        "dataset = load_math_dataset()\n",
        "\n",
        "print(f\"Train set size: {len(dataset['train'])}\")\n",
        "print(f\"Test set size: {len(dataset['test'])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdAvBnP-VgrK",
        "outputId": "bae213be-6234-4246-c3be-e8cbd3d16c67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Validating train split:\n",
            "‚úì All required fields present\n",
            "‚úì Prompt format is correct\n",
            "\n",
            "Validating test split:\n",
            "‚úì All required fields present\n",
            "‚úì Prompt format is correct\n"
          ]
        }
      ],
      "source": [
        "def validate_dataset(dataset):\n",
        "    \"\"\"Perform basic validation checks on the dataset.\"\"\"\n",
        "\n",
        "    # Define the required fields for the dataset\n",
        "    required_fields = [\"problem\", \"prompt\"]\n",
        "\n",
        "    # Loop through the 'train' and 'test' splits of the dataset\n",
        "    for split in ['train', 'test']:\n",
        "        print(f\"\\nValidating {split} split:\")\n",
        "\n",
        "        # Retrieve column names from the dataset\n",
        "        fields = dataset[split].column_names\n",
        "\n",
        "        # Check if any required fields are missing\n",
        "        missing = [field for field in required_fields if field not in fields]\n",
        "        if missing:\n",
        "            print(f\"Warning: Missing fields: {missing}\")  # Warn if fields are missing\n",
        "        else:\n",
        "            print(\"‚úì All required fields present\")  # Confirm all fields are present\n",
        "\n",
        "        # Retrieve the first sample from the dataset split\n",
        "        sample = dataset[split][0]\n",
        "\n",
        "        # Extract the 'prompt' field, which contains a list of messages\n",
        "        messages = sample['prompt']\n",
        "\n",
        "        # Validate the prompt format:\n",
        "        # - It should contain at least two messages\n",
        "        # - The first message should be from the 'system' role\n",
        "        # - The second message should be from the 'user' role\n",
        "        if (len(messages) >= 2 and\n",
        "            messages[0]['role'] == 'system' and\n",
        "            messages[1]['role'] == 'user'):\n",
        "            print(\"‚úì Prompt format is correct\")  # Confirm correct format\n",
        "        else:\n",
        "            print(\"Warning: Incorrect prompt format\")  # Warn if format is incorrect\n",
        "\n",
        "# Validate dataset\n",
        "validate_dataset(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "xUbz3WIiVgrL"
      },
      "outputs": [],
      "source": [
        "def accuracy_reward(completions, **kwargs):\n",
        "    \"\"\"\n",
        "    Reward function to check if the model's response is mathematically\n",
        "    equivalent to the ground truth solution.\n",
        "    Uses latex2sympy2 for parsing and math_verify for validation.\n",
        "    \"\"\"\n",
        "\n",
        "    # Extract responses\n",
        "    contents = [completion[0][\"content\"] for completion in completions]\n",
        "    rewards = []\n",
        "\n",
        "    solutions = kwargs.get(\"solution\") # Get solutions from kwargs\n",
        "\n",
        "    for content, sol in zip(contents, solutions):\n",
        "        # Parse the ground truth solution\n",
        "        gold_parsed = parse(sol, extraction_mode=\"first_match\",\n",
        "                            extraction_config=[LatexExtractionConfig()])\n",
        "\n",
        "        if gold_parsed:  # Check if parsing was successful\n",
        "            # Parse the model's answer with relaxed normalization\n",
        "            answer_parsed = parse(\n",
        "                content,\n",
        "                extraction_config=[\n",
        "                    LatexExtractionConfig(\n",
        "                        normalization_config=NormalizationConfig(\n",
        "                            nits=False,\n",
        "                            malformed_operators=False,\n",
        "                            basic_latex=True,\n",
        "                            # equations=True, # equations argument is deprecated\n",
        "                            boxed=\"all\",\n",
        "                            units=True,\n",
        "                        ),\n",
        "                        boxed_match_priority=0,\n",
        "                        try_extract_without_anchor=False,\n",
        "                    )\n",
        "                ],\n",
        "                extraction_mode=\"first_match\",\n",
        "            )\n",
        "\n",
        "            # Reward 1.0 if correct, 0.0 if incorrect\n",
        "            reward = float(verify(answer_parsed, gold_parsed))\n",
        "        else:\n",
        "            # If ground truth cannot be parsed, assign neutral reward (0.5)\n",
        "            reward = 0.5\n",
        "            print(\"Warning: Failed to parse gold solution:\", sol)\n",
        "\n",
        "        rewards.append(reward)\n",
        "\n",
        "    # Log the rewards in log dict\n",
        "    log_dict[\"accuracy_reward\"].extend(rewards)\n",
        "\n",
        "    return rewards\n",
        "\n",
        "# Implement Format Reward Function\n",
        "def format_reward(completions, **kwargs):\n",
        "    \"\"\"\n",
        "    Reward function to check if the completion has the correct format:\n",
        "    <think>...</think> <answer>...</answer>.\n",
        "    \"\"\"\n",
        "    # Define the regex pattern for the desired format\n",
        "    pattern = r\"^<think>.*?</think>\\s*<answer>.*?</answer>$\"\n",
        "\n",
        "    # Extract the content from each completion\n",
        "    completion_contents = [completion[0][\"content\"] for completion in completions]\n",
        "\n",
        "    # Check if each completion matches the pattern\n",
        "    matches = [re.match(pattern, content, re.DOTALL | re.MULTILINE)\n",
        "               for content in completion_contents]\n",
        "\n",
        "    # Reward 1.0 for correct format, 0.0 otherwise\n",
        "    rewards = [1.0 if match else 0.0 for match in matches]\n",
        "\n",
        "    # Log the rewards in log dict\n",
        "    log_dict[\"format_reward\"].extend(rewards)\n",
        "\n",
        "    return rewards\n",
        "\n",
        "def reasoning_steps_reward(completions, **kwargs):\n",
        "    r\"\"\"\n",
        "    Reward function to encourage clear step-by-step reasoning.\n",
        "    It looks for patterns like \"Step 1:\", numbered lists, bullet points,\n",
        "    and transition words.\n",
        "    \"\"\"\n",
        "    # Regex pattern to find indicators of reasoning steps\n",
        "    pattern = r\"(Step \\d+:|^\\d+\\.|\\n-|\\n\\*|First,|Second,|Next,|Finally,)\"\n",
        "\n",
        "    # Extract completion contents\n",
        "    completion_contents = [completion[0][\"content\"] for completion in completions]\n",
        "\n",
        "    # Count the number of reasoning step indicators in each completion\n",
        "    matches = [len(re.findall(pattern, content, re.MULTILINE))\n",
        "               for content in completion_contents]\n",
        "\n",
        "    # Reward is proportional to the number of reasoning steps, maxing out at 1.0\n",
        "    # We're using a \"magic number\" 3 here - encourage at least 3 steps for full reward\n",
        "    rewards = [min(1.0, count / 3) for count in matches]\n",
        "\n",
        "    # Log the rewards in log dict\n",
        "    log_dict[\"reasoning_steps_reward\"].extend(rewards)\n",
        "\n",
        "    return rewards\n",
        "\n",
        "# Implement Cosine Scaled Reward Function\n",
        "def get_cosine_scaled_reward(\n",
        "    min_value_wrong: float = -0.5,\n",
        "    max_value_wrong: float = -0.1,\n",
        "    min_value_correct: float = 0.8,\n",
        "    max_value_correct: float = 1.0,\n",
        "    max_len: int = 1000,\n",
        "):\n",
        "    \"\"\"\n",
        "    Returns a cosine scaled reward function. This function scales the accuracy reward\n",
        "    based on completion length. Shorter correct solutions get higher rewards,\n",
        "    longer incorrect solutions get less penalty.\n",
        "    \"\"\"\n",
        "    def cosine_scaled_reward(completions, solution, accuracy_rewards, **kwargs):\n",
        "        \"\"\"\n",
        "        Cosine scaled reward function that adjusts accuracy rewards based on completion length.\n",
        "        \"\"\"\n",
        "        contents = [completion[0][\"content\"] for completion in completions]\n",
        "        rewards = []\n",
        "\n",
        "        for content, sol, acc_reward in zip(contents, solution, accuracy_rewards):\n",
        "            gen_len = len(content)  # Length of the generated answer\n",
        "            progress = gen_len / max_len # How far we are to max length\n",
        "            cosine = math.cos(progress * math.pi) # Cosine value based on progress\n",
        "\n",
        "            if acc_reward > 0.5: # Assuming accuracy_reward gives ~1.0 for correct answers\n",
        "                min_value = min_value_correct\n",
        "                max_value = max_value_correct\n",
        "            else: # Incorrect answer\n",
        "                min_value = max_value_wrong  # Note the swap!\n",
        "                max_value = min_value_wrong\n",
        "\n",
        "            # Cosine scaling formula!\n",
        "            reward = min_value + 0.5 * (max_value - min_value) * (1.0 + cosine)\n",
        "            rewards.append(float(reward))\n",
        "\n",
        "        # Log the rewards in log dict\n",
        "        log_dict[\"cosine_scaled_reward\"].extend(rewards)\n",
        "\n",
        "        return rewards\n",
        "    \n",
        "    return cosine_scaled_reward\n",
        "\n",
        "def get_repetition_penalty_reward(ngram_size: int = 3, max_penalty: float = -0.1):\n",
        "    \"\"\"\n",
        "    Returns a repetition penalty reward function. Penalizes repetitions of n-grams\n",
        "    in the generated text.\n",
        "    \"\"\"\n",
        "    if max_penalty > 0:\n",
        "        raise ValueError(f\"max_penalty {max_penalty} should not be positive\")\n",
        "\n",
        "    def zipngram(text: str, ngram_size: int):\n",
        "        \"\"\"Helper function to generate n-grams from text.\"\"\"\n",
        "        words = text.lower().split() # Lowercase and split into words\n",
        "        return zip(*[words[i:] for i in range(ngram_size)]) # Create n-grams\n",
        "\n",
        "    def repetition_penalty_reward(completions, **kwargs) -> float:\n",
        "        \"\"\"\n",
        "        Repetition penalty reward function.\n",
        "        \"\"\"\n",
        "        contents = [completion[0][\"content\"] for completion in completions]\n",
        "        rewards = []\n",
        "        for completion in contents:\n",
        "            if completion == \"\": # No penalty for empty completions\n",
        "                rewards.append(0.0)\n",
        "                continue\n",
        "            if len(completion.split()) < ngram_size: # No penalty for short completions\n",
        "                rewards.append(0.0)\n",
        "                continue\n",
        "\n",
        "            ngrams = set() # Use a set to store unique n-grams\n",
        "            total = 0\n",
        "            for ng in zipngram(completion, ngram_size): # Generate n-grams\n",
        "                ngrams.add(ng) # Add n-gram to the set (duplicates are ignored)\n",
        "                total += 1 # Count total n-grams\n",
        "\n",
        "            # Calculate scaling factor: more repetition -> higher scaling\n",
        "            scaling = 1 - len(ngrams) / total\n",
        "            reward = scaling * max_penalty # Apply penalty based on scaling\n",
        "            rewards.append(reward)\n",
        "\n",
        "        # Log the rewards in log dict\n",
        "        log_dict[\"repetition_penalty_reward\"].extend(rewards)\n",
        "\n",
        "        return rewards\n",
        "    \n",
        "    return repetition_penalty_reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jUOS2A-BVgrb"
      },
      "outputs": [],
      "source": [
        "# Define GRPOScriptArguments for reward function parameters\n",
        "@dataclass\n",
        "class GRPOScriptArguments:\n",
        "    \"\"\"\n",
        "    Script arguments for GRPO training, specifically related to reward functions.\n",
        "    \"\"\"\n",
        "\n",
        "    reward_funcs: list[str] = field(\n",
        "        default_factory=lambda: [\"accuracy\", \"format\"],\n",
        "        metadata={\n",
        "            \"help\": \"List of reward functions. Possible values: 'accuracy', 'format', 'reasoning_steps', 'cosine', 'repetition_penalty'\"\n",
        "        },\n",
        "    )\n",
        "    cosine_min_value_wrong: float = field(\n",
        "        default=-0.5,\n",
        "        metadata={\"help\": \"Minimum reward for cosine scaling for wrong answers\"},\n",
        "    )\n",
        "    cosine_max_value_wrong: float = field(\n",
        "        default=-0.1,\n",
        "        metadata={\"help\": \"Maximum reward for cosine scaling for wrong answers\"},\n",
        "    )\n",
        "    cosine_min_value_correct: float = field(\n",
        "        default=0.8,\n",
        "        metadata={\"help\": \"Minimum reward for cosine scaling for correct answers\"},\n",
        "    )\n",
        "    cosine_max_value_correct: float = field(\n",
        "        default=1.0,\n",
        "        metadata={\"help\": \"Maximum reward for cosine scaling for correct answers\"},\n",
        "    )\n",
        "    cosine_max_len: int = field(\n",
        "        default=1000,\n",
        "        metadata={\"help\": \"Maximum length for cosine scaling\"},\n",
        "    )\n",
        "\n",
        "    repetition_n_grams: int = field(\n",
        "        default=3,\n",
        "        metadata={\"help\": \"Number of n-grams for repetition penalty reward\"},\n",
        "    )\n",
        "    repetition_max_penalty: float = field(\n",
        "        default=-0.1,\n",
        "        metadata={\"help\": \"Maximum (negative) penalty for for repetition penalty reward\"},\n",
        "    )\n",
        "\n",
        "@dataclass\n",
        "class ModelConfig:\n",
        "    \"\"\"\n",
        "    Configuration for the model.\n",
        "    \"\"\"\n",
        "    model_name_or_path: str = field(\n",
        "        default=MODEL_NAME, metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n",
        "    )\n",
        "    model_revision: Optional[str] = field(\n",
        "        default=\"main\", metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"}\n",
        "    )\n",
        "    torch_dtype: Optional[str] = field(\n",
        "        default=\"bfloat16\", metadata={\"help\": \"Override the default `torch_dtype` and load the model under this dtype.\"}\n",
        "    )\n",
        "    trust_remote_code: bool = field(\n",
        "        default=True, metadata={\"help\": \"Trust remote code when loading model and tokenizer.\"}\n",
        "    )\n",
        "    attn_implementation: Optional[str] = field(\n",
        "        default=\"flash_attention_2\", metadata={\"help\": \"Attention implementation to use. 'flash_attention_2' or None\"}\n",
        "    )\n",
        "\n",
        "# Define TrainingArguments from transformers\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,          # Output directory for checkpoints and logs\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=1,             # Total number of training epochs\n",
        "    per_device_train_batch_size=8,  # Batch size per device during training\n",
        "    per_device_eval_batch_size=16,   # Batch size for evaluation\n",
        "    gradient_accumulation_steps=2,  # Accumulate gradients to simulate larger batch size\n",
        "    learning_rate=5e-5,            # Initial learning rate for AdamW optimizer\n",
        "    warmup_ratio=0.1,              # Linear warmup over warmup_ratio fraction of training steps\n",
        "    weight_decay=0.01,             # Apply weight decay to all layers except bias and LayerNorm weights\n",
        "    logging_steps=2,              # Log every X updates steps\n",
        "    eval_strategy=\"steps\",         # Evaluate every `eval_steps`\n",
        "    eval_steps=50,                 # Evaluation and logging steps\n",
        "    save_strategy=\"steps\",         # Save checkpoint every `save_steps`\n",
        "    save_steps=50,                 # Save checkpoint every X updates steps\n",
        "    save_total_limit=2,            # Limit the total amount of checkpoints. Deletes the older checkpoints.\n",
        "    # dataloader_num_workers=2,      # Number of subprocesses to use for data loading\n",
        "    seed=42,                       # Random seed for reproducibility\n",
        "    bf16=True,                     # Use mixed precision BFP16 training\n",
        "    push_to_hub=False,             # Whether to push the final model to Hugging Face Hub\n",
        "    gradient_checkpointing=True,   # Enable gradient checkpointing\n",
        "    report_to=[],                  # Reporting to no one\n",
        "    remove_unused_columns=False,   # Do not remove unused columns from the dataset\n",
        ")\n",
        "\n",
        "# Instantiate configuration objects\n",
        "script_args = GRPOScriptArguments()\n",
        "model_args = ModelConfig()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "_Wv_6v71Vgrf"
      },
      "outputs": [],
      "source": [
        "# Utility function to get reward functions based on script arguments\n",
        "def get_reward_functions(script_args):\n",
        "    \"\"\"\n",
        "    Returns a list of reward functions based on the script arguments.\n",
        "    \"\"\"\n",
        "    reward_funcs_list = []\n",
        "    reward_funcs_registry = {\n",
        "        \"accuracy\": accuracy_reward,  # Assuming accuracy_reward is defined in previous steps\n",
        "        \"format\": format_reward,      # Assuming format_reward is defined in previous steps\n",
        "        \"reasoning_steps\": reasoning_steps_reward, # Assuming reasoning_steps_reward is defined\n",
        "        \"cosine\": get_cosine_scaled_reward( # Assuming get_cosine_scaled_reward is defined\n",
        "            min_value_wrong=script_args.cosine_min_value_wrong,\n",
        "            max_value_wrong=script_args.cosine_max_value_wrong,\n",
        "            min_value_correct=script_args.cosine_min_value_correct,\n",
        "            max_value_correct=script_args.cosine_max_value_correct,\n",
        "            max_len=script_args.cosine_max_len,\n",
        "        ),\n",
        "        \"repetition_penalty\": get_repetition_penalty_reward( # Assuming get_repetition_penalty_reward is defined\n",
        "            ngram_size=script_args.repetition_n_grams,\n",
        "            max_penalty=script_args.repetition_max_penalty,\n",
        "        ),\n",
        "    }\n",
        "\n",
        "    for func_name in script_args.reward_funcs:\n",
        "        if func_name not in reward_funcs_registry:\n",
        "            raise ValueError(f\"Reward function '{func_name}' not found in registry.\")\n",
        "        reward_funcs_list.append(reward_funcs_registry[func_name])\n",
        "\n",
        "    return reward_funcs_list\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class LoggingCallback(TrainerCallback):\n",
        "    \"\"\"\n",
        "    A simple callback for logging training information at specific steps.\n",
        "    \"\"\"\n",
        "    def on_step_end(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n",
        "        if state.global_step % args.logging_steps == 0:\n",
        "            if state.log_history and len(state.log_history) > 0:\n",
        "                logger.info(f\"Step {state.global_step}: Loss = {state.log_history[-1].get('loss', None)}, Learning Rate = {state.log_history[-1].get('learning_rate', None)}\")\n",
        "            else:\n",
        "                logger.info(f\"Step {state.global_step}: No logging information available yet\")\n",
        "            \n",
        "            batch_size = args.per_device_train_batch_size * args.gradient_accumulation_steps\n",
        "            current_idx = (state.global_step - args.logging_steps) * batch_size\n",
        "            accuracy_reward = np.mean(log_dict[\"accuracy_reward\"][current_idx:])\n",
        "            format_reward = np.mean(log_dict[\"format_reward\"][current_idx:])\n",
        "            reasoning_steps_reward = np.mean(log_dict[\"reasoning_steps_reward\"][current_idx:])\n",
        "            cosine_scaled_reward = np.mean(log_dict[\"cosine_scaled_reward\"][current_idx:])\n",
        "            repetition_penalty_reward = np.mean(log_dict[\"repetition_penalty_reward\"][current_idx:])\n",
        "            rewards = [reward for reward in [batch_size, accuracy_reward, format_reward, \\\n",
        "                                             reasoning_steps_reward, cosine_scaled_reward, \\\n",
        "                                                repetition_penalty_reward] if reward]\n",
        "            total_reward = sum(rewards)\n",
        "\n",
        "            # log to wandb\n",
        "            wandb.log({\n",
        "                \"step\": state.global_step,\n",
        "                \"train_loss\": state.log_history[-1][\"loss\"],\n",
        "                \"learning_rate\": state.log_history[-1][\"learning_rate\"],\n",
        "                \"accuracy_reward\": accuracy_reward,\n",
        "                \"format_reward\": format_reward,\n",
        "                \"reasoning_steps_reward\": reasoning_steps_reward,\n",
        "                \"cosine_scaled_reward\": cosine_scaled_reward,\n",
        "                \"repetition_penalty_reward\": repetition_penalty_reward,\n",
        "                \"total_reward\": total_reward\n",
        "            })\n",
        "\n",
        "def get_callbacks(training_args, model_args, script_args):\n",
        "    \"\"\"\n",
        "    Returns a list of callbacks to be used during training.\n",
        "    For now, it includes only the LoggingCallback. You can extend this to add more callbacks.\n",
        "    \"\"\"\n",
        "    callbacks = [LoggingCallback()] # Instantiate our LoggingCallback\n",
        "    return callbacks\n",
        "\n",
        "# Get reward functions and callbacks\n",
        "reward_functions = get_reward_functions(script_args)\n",
        "callbacks = get_callbacks(training_args, model_args, script_args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# class CustomGRPOTrainer(GRPOTrainer):\n",
        "#     def log(self, logs: dict, start_time: float = None):\n",
        "#         print(logs)\n",
        "\n",
        "#         if hasattr(self, \"state\"):\n",
        "#             step = self.state.global_step\n",
        "#             logs[\"step\"] = step\n",
        "#         print(logs)\n",
        "#         print(\"---------------\")\n",
        "\n",
        "#         if logs:\n",
        "#             wandb.log(logs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SKDIAj6wVgrg",
        "outputId": "03e528d2-9f0e-436a-a7a0-2ef5daccd794"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/workspace/DeepSeekR1Deployment/.venv/lib/python3.11/site-packages/transformers/training_args.py:2029: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ü§ó Transformers. Use `--hub_token` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/workspace/DeepSeekR1Deployment/wandb/run-20250629_085820-cts4pdxn</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/magn3144/DeepSeekR1Training/runs/cts4pdxn' target=\"_blank\">test_run_1</a></strong> to <a href='https://wandb.ai/magn3144/DeepSeekR1Training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/magn3144/DeepSeekR1Training' target=\"_blank\">https://wandb.ai/magn3144/DeepSeekR1Training</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/magn3144/DeepSeekR1Training/runs/cts4pdxn' target=\"_blank\">https://wandb.ai/magn3144/DeepSeekR1Training/runs/cts4pdxn</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
            "/workspace/DeepSeekR1Deployment/.venv/lib/python3.11/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2/5 : < :, Epoch 0.20/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/workspace/DeepSeekR1Deployment/.venv/lib/python3.11/site-packages/numpy/_core/fromnumeric.py:3860: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/workspace/DeepSeekR1Deployment/.venv/lib/python3.11/site-packages/numpy/_core/_methods.py:144: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        },
        {
          "ename": "IndexError",
          "evalue": "list index out of range",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     21\u001b[39m run = wandb.init(\n\u001b[32m     22\u001b[39m     project=\u001b[33m\"\u001b[39m\u001b[33mDeepSeekR1Training\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     23\u001b[39m     name=\u001b[33m\"\u001b[39m\u001b[33mtest_run_1\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     27\u001b[39m     },\n\u001b[32m     28\u001b[39m )\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# Start the GRPO Training Loop\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m train_result = \u001b[43mgrpo_trainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m sys.exit()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/workspace/DeepSeekR1Deployment/.venv/lib/python3.11/site-packages/transformers/trainer.py:2240\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2238\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2239\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2240\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2241\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2242\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2243\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2244\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2245\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/workspace/DeepSeekR1Deployment/.venv/lib/python3.11/site-packages/transformers/trainer.py:2621\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2619\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.global_step += \u001b[32m1\u001b[39m\n\u001b[32m   2620\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.epoch = epoch + (step + \u001b[32m1\u001b[39m + steps_skipped) / steps_in_epoch\n\u001b[32m-> \u001b[39m\u001b[32m2621\u001b[39m     \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcallback_handler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mon_step_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcontrol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2622\u001b[39m     \u001b[38;5;28mself\u001b[39m._maybe_log_save_evaluate(\n\u001b[32m   2623\u001b[39m         tr_loss,\n\u001b[32m   2624\u001b[39m         grad_norm,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2630\u001b[39m         learning_rate=learning_rate,\n\u001b[32m   2631\u001b[39m     )\n\u001b[32m   2632\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/workspace/DeepSeekR1Deployment/.venv/lib/python3.11/site-packages/transformers/trainer_callback.py:534\u001b[39m, in \u001b[36mCallbackHandler.on_step_end\u001b[39m\u001b[34m(self, args, state, control)\u001b[39m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mon_step_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: TrainingArguments, state: TrainerState, control: TrainerControl):\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_event\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mon_step_end\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontrol\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/workspace/DeepSeekR1Deployment/.venv/lib/python3.11/site-packages/transformers/trainer_callback.py:556\u001b[39m, in \u001b[36mCallbackHandler.call_event\u001b[39m\u001b[34m(self, event, args, state, control, **kwargs)\u001b[39m\n\u001b[32m    554\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_event\u001b[39m(\u001b[38;5;28mself\u001b[39m, event, args, state, control, **kwargs):\n\u001b[32m    555\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.callbacks:\n\u001b[32m--> \u001b[39m\u001b[32m556\u001b[39m         result = \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[43m            \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    558\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    559\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcontrol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    560\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    561\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[43m            \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    564\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    565\u001b[39m \u001b[43m            \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    566\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    568\u001b[39m         \u001b[38;5;66;03m# A Callback can skip the return of `control` if it doesn't change it.\u001b[39;00m\n\u001b[32m    569\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 59\u001b[39m, in \u001b[36mLoggingCallback.on_step_end\u001b[39m\u001b[34m(self, args, state, control, **kwargs)\u001b[39m\n\u001b[32m     54\u001b[39m total_reward = \u001b[38;5;28msum\u001b[39m(rewards)\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# log to wandb\u001b[39;00m\n\u001b[32m     57\u001b[39m wandb.log({\n\u001b[32m     58\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mstep\u001b[39m\u001b[33m\"\u001b[39m: state.global_step,\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtrain_loss\u001b[39m\u001b[33m\"\u001b[39m: \u001b[43mstate\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlog_history\u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     60\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mlearning_rate\u001b[39m\u001b[33m\"\u001b[39m: state.log_history[-\u001b[32m1\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mlearning_rate\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     61\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33maccuracy_reward\u001b[39m\u001b[33m\"\u001b[39m: accuracy_reward,\n\u001b[32m     62\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mformat_reward\u001b[39m\u001b[33m\"\u001b[39m: format_reward,\n\u001b[32m     63\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreasoning_steps_reward\u001b[39m\u001b[33m\"\u001b[39m: reasoning_steps_reward,\n\u001b[32m     64\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcosine_scaled_reward\u001b[39m\u001b[33m\"\u001b[39m: cosine_scaled_reward,\n\u001b[32m     65\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mrepetition_penalty_reward\u001b[39m\u001b[33m\"\u001b[39m: repetition_penalty_reward,\n\u001b[32m     66\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtotal_reward\u001b[39m\u001b[33m\"\u001b[39m: total_reward\n\u001b[32m     67\u001b[39m })\n",
            "\u001b[31mIndexError\u001b[39m: list index out of range"
          ]
        }
      ],
      "source": [
        "# Create GRPOConfig from TrainingArguments\n",
        "grpo_config = GRPOConfig(\n",
        "    **training_args.to_dict(), # Convert TrainingArguments to dictionary and unpack\n",
        "    **{\n",
        "       # REMOVED model_init_kwargs here\n",
        "       # We are passing the instantiated 'model' object, so GRPOTrainer doesn't need model_init_kwargs\n",
        "    }\n",
        ")\n",
        "\n",
        "train_size = 10\n",
        "test_size = 1\n",
        "grpo_trainer = GRPOTrainer(\n",
        "    model=model,                      # Our initialized Qwen model\n",
        "    reward_funcs=reward_functions,    # List of reward functions from previous step\n",
        "    args=grpo_config,                # GRPOConfig (created from TrainingArguments)\n",
        "    train_dataset=dataset['train'].select(range(train_size)),   # Training dataset\n",
        "    eval_dataset=dataset['test'].select(range(test_size)),    # Evaluation dataset\n",
        "    callbacks=callbacks              # List of callbacks\n",
        ")\n",
        "\n",
        "run = wandb.init(\n",
        "    project=\"DeepSeekR1Training\",\n",
        "    name=\"test_run_1\",\n",
        "    config={\n",
        "        \"learning_rate\": training_args.learning_rate,\n",
        "        \"epochs\": training_args.num_train_epochs,\n",
        "    },\n",
        ")\n",
        "\n",
        "# Start the GRPO Training Loop\n",
        "train_result = grpo_trainer.train()\n",
        "\n",
        "sys.exit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "aLbq9FzXVgri"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GRPO Trained model saved to data/Qwen-GRPO-training\n"
          ]
        }
      ],
      "source": [
        "# Define the path to your trained model (same as OUTPUT_DIR)\n",
        "TRAINED_MODEL_PATH = \"data/Qwen-GRPO-training\"\n",
        "\n",
        "# Save the tokenizer\n",
        "tokenizer.save_pretrained(TRAINED_MODEL_PATH)\n",
        "\n",
        "# Save the trained model\n",
        "grpo_trainer.save_model(TRAINED_MODEL_PATH)\n",
        "\n",
        "print(f\"GRPO Trained model saved to {TRAINED_MODEL_PATH}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "tbjoV4eGVgri"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Qwen2ForCausalLM(\n",
              "  (model): Qwen2Model(\n",
              "    (embed_tokens): Embedding(151936, 896)\n",
              "    (layers): ModuleList(\n",
              "      (0-23): 24 x Qwen2DecoderLayer(\n",
              "        (self_attn): Qwen2Attention(\n",
              "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
              "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
              "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
              "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
              "        )\n",
              "        (mlp): Qwen2MLP(\n",
              "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
              "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
              "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
              "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
              "      )\n",
              "    )\n",
              "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
              "    (rotary_emb): Qwen2RotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load the tokenizer - make sure to use trust_remote_code=True if needed\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    TRAINED_MODEL_PATH,\n",
        "    trust_remote_code=True, # If your model config requires it\n",
        "    padding_side=\"right\" # Ensure consistent padding side\n",
        ")\n",
        "\n",
        "# Set pad token if it wasn't saved or loaded correctly\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Load the trained model itself\n",
        "trained_model = AutoModelForCausalLM.from_pretrained(\n",
        "    TRAINED_MODEL_PATH,\n",
        "    trust_remote_code=True, # If your model architecture requires it\n",
        "    torch_dtype=torch.bfloat16 # Keep the same dtype as training for consistency\n",
        ")\n",
        "\n",
        "# Move the loaded model to your device (GPU if available)\n",
        "trained_model.to(device) # 'device' is still our CUDA device from before"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "3uMLRtD8Vgrj"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Input: how are you?\n",
            "Trained Model Response: system\n",
            "A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think><answer> answer here </answer>\n",
            "user\n",
            "how are you?\n",
            "assistant\n",
            " In this task, we need to determine how the user is responding to each other's messages (in this case, an AI response). We must analyze the content of both the input messages and the output message individually to understand their relationship.\n",
            "\n",
            "1. First, I read through the provided text carefully, focusing on the dialogue between User and Assistant.\n",
            "\n",
            "2. Next, I identified that the conversation is between two individuals: User and Assistant. Both users are engaged in a discussion.\n",
            "\n",
            "3. After understanding the context, I analyzed the responses made by each individual:\n",
            "\n",
            "   - For User, the response is \"how are you?\" which indicates that they are seeking information from the Assistant regarding their state of well-being or mood.\n",
            "   \n",
            "   - For Assistant, the response is \"how are you?\" as well, which also expresses interest in knowing more about User's current condition.\n",
            "\n",
            "4. By analyzing these interactions, I determined that the user is asking a general inquiry (\"how are you?\") while the assistant responds similarly\n"
          ]
        }
      ],
      "source": [
        "# Testing Inference with the Trained Model\n",
        "def test_trained_model_inference(user_input: str):\n",
        "    \"\"\"Test inference with the loaded trained model and tokenizer.\"\"\"\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT}, # Re-use our system prompt\n",
        "        {\"role\": \"user\", \"content\": user_input}\n",
        "    ]\n",
        "\n",
        "    # Apply chat template using our tokenizer\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "\n",
        "    # Tokenize the input text\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # Generate output using our *trained_model*\n",
        "    outputs = trained_model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=200, # Maybe generate a bit longer now\n",
        "        do_sample=True,\n",
        "        temperature=0.7\n",
        "    )\n",
        "\n",
        "    # Decode the generated tokens back to text\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response\n",
        "\n",
        "# Test the model\n",
        "test_input = \"how are you?\"\n",
        "response = test_trained_model_inference(test_input)\n",
        "print(f\"Test Input: {test_input}\")\n",
        "print(f\"Trained Model Response: {response}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "a4BqqXjZVgrm"
      },
      "outputs": [],
      "source": [
        "# # Load the \"Bespoke-Stratos-17k\" dataset from bespokelabs\n",
        "# bespoke_rl = load_dataset(\"bespokelabs/Bespoke-Stratos-17k\", \"default\")\n",
        "\n",
        "# # Access the first sample in the training set\n",
        "# bespoke_rl['train'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Wk4KWg_mVgrm"
      },
      "outputs": [],
      "source": [
        "# Model and Output Configuration (same as before, or adjust as needed)\n",
        "MODEL_NAME = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
        "OUTPUT_DIR = \"data/Qwen-SFT-training\" # New output directory for SFT model\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Training Arguments - similar to GRPO, but adjust for SFT\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=1,         # Adjust epochs as needed\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=16,\n",
        "    gradient_accumulation_steps=2,\n",
        "    learning_rate=2e-5,        # Adjust learning rate for SFT\n",
        "    warmup_ratio=0.1,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=10,\n",
        "    # evaluation_strategy=\"no\",\n",
        "    eval_steps=50,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=50,\n",
        "    save_total_limit=2,\n",
        "    dataloader_num_workers=2,\n",
        "    seed=42,\n",
        "    bf16=True,\n",
        "    push_to_hub=False,\n",
        "    gradient_checkpointing=True,\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "# Model Configuration - same as before\n",
        "model_args = ModelConfig(\n",
        "    model_name_or_path=MODEL_NAME,\n",
        "    model_revision=\"main\",\n",
        "    torch_dtype=\"bfloat16\",\n",
        "    trust_remote_code=True,\n",
        "    attn_implementation=\"flash_attention_2\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "H7OiNtxCVgrn"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating train split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16610/16610 [00:01<00:00, 16470.30 examples/s]\n",
            "Generating test split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:00<00:00, 17865.59 examples/s]\n"
          ]
        }
      ],
      "source": [
        "# Load Bespoke-Stratos-17k dataset\n",
        "dataset_sft = load_dataset(\"HuggingFaceH4/Bespoke-Stratos-17k\", split='train') # Only using train split for simplicity\n",
        "\n",
        "# Initialize tokenizer - same as before\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    trust_remote_code=True,\n",
        "    padding_side=\"right\"\n",
        ")\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'system': \"Your role as an assistant involves thoroughly exploring questions through a systematic long thinking process before providing the final precise and accurate solutions. This requires engaging in a comprehensive cycle of analysis, summarizing, exploration, reassessment, reflection, backtracing, and iteration to develop well-considered thinking process. Please structure your response into two main sections: Thought and Solution. In the Thought section, detail your reasoning process using the specified format: <|begin_of_thought|> {thought with steps separated with '\\\\n\\\\n'} <|end_of_thought|> Each step should include detailed considerations such as analisying questions, summarizing relevant findings, brainstorming new ideas, verifying the accuracy of the current steps, refining any errors, and revisiting previous steps. In the Solution section, based on various attempts, explorations, and reflections from the Thought section, systematically present the final solution that you deem correct. The solution should remain a logical, accurate, concise expression style and detail necessary step needed to reach the conclusion, formatted as follows: <|begin_of_solution|> {final formatted, precise, and clear solution} <|end_of_solution|> Now, try to solve the following question through the above guidelines:\",\n",
              " 'conversations': [{'from': 'user',\n",
              "   'value': \"Return your final response within \\\\boxed{}. Ms. Blackwell gives an exam to two classes. The mean of the scores of the students in the morning class is $84$, and the afternoon class's mean score is $70$. The ratio of the number of students in the morning class to the number of students in the afternoon class is $\\\\frac{3}{4}$. What is the mean of the scores of all the students?\\n$\\\\textbf{(A)} ~74 \\\\qquad\\\\textbf{(B)} ~75 \\\\qquad\\\\textbf{(C)} ~76 \\\\qquad\\\\textbf{(D)} ~77 \\\\qquad\\\\textbf{(E)} ~78$\"},\n",
              "  {'from': 'assistant',\n",
              "   'value': \"<|begin_of_thought|>\\n\\nOkay, let's see. I need to find the mean score of all the students combined from both the morning and afternoon classes. Hmm, the problem gives me the means of each class and the ratio of the number of students. Let me jot down the details:\\n\\n- Morning class mean: 84\\n- Afternoon class mean: 70\\n- Ratio of students (Morning:Afternoon): 3:4\\n\\nSo, I remember that the overall mean is calculated by the total sum of all scores divided by the total number of students. Right. So, if I can find the total sum from both classes and divide by the total number of students, that should give me the mean.\\n\\nBut since I don't know the exact number of students, just the ratio, maybe I can use variables to represent them. Let's let the number of students in the morning class be 3k and the afternoon class be 4k, where k is some positive integer. That way, the ratio is maintained as 3:4. Good idea.\\n\\nNow, the total sum for the morning class would be the mean multiplied by the number of students. So, that's 84 * 3k. Similarly, the total sum for the afternoon class is 70 * 4k. Let me calculate those:\\n\\nMorning total = 84 * 3k = 252k\\n\\nAfternoon total = 70 * 4k = 280k\\n\\nSo, combined total score is 252k + 280k = 532k\\n\\nTotal number of students is 3k + 4k = 7k\\n\\nTherefore, the overall mean is total score divided by total students: 532k / 7k\\n\\nWait, the k cancels out here. So, 532 / 7. Let me compute that.\\n\\nDividing 532 by 7. Let's see, 7*70=490. 532 - 490 = 42. Then 42/7=6. So total is 70 + 6 = 76. So, 76 is the mean.\\n\\nLooking back at the options, choice C is 76. So, that's the answer. Let me just verify once again to make sure I didn't make any calculation mistakes.\\n\\nMean of morning: 84, students 3k. Total morning: 84*3k=252k. Correct.\\n\\nMean of afternoon:70, students 4k. Total afternoon:70*4k=280k. Correct.\\n\\nTotal combined: 252k + 280k = 532k. Total students:7k. 532k /7k = 76. Yep, that's right. The ratio was 3:4, so using 3k and 4k works because k cancels. So regardless of the actual number of students, as long as the ratio is 3:4, the mean will be 76. That makes sense.\\n\\nSo, the answer is C) 76.\\n\\n**Final Answer**\\n\\\\boxed{C}\\n\\n<|end_of_thought|>\\n\\n<|begin_of_solution|>\\n\\nTo find the mean of the scores of all the students combined from both the morning and afternoon classes, we start by noting the given information:\\n\\n- The mean score of the morning class is 84.\\n- The mean score of the afternoon class is 70.\\n- The ratio of the number of students in the morning class to the afternoon class is \\\\( \\\\frac{3}{4} \\\\).\\n\\nWe represent the number of students in the morning class as \\\\( 3k \\\\) and the number of students in the afternoon class as \\\\( 4k \\\\), where \\\\( k \\\\) is a positive integer. \\n\\nNext, we calculate the total sum of scores for each class:\\n- Total sum for the morning class: \\\\( 84 \\\\times 3k = 252k \\\\)\\n- Total sum for the afternoon class: \\\\( 70 \\\\times 4k = 280k \\\\)\\n\\nThe combined total score of all students is:\\n\\\\[ 252k + 280k = 532k \\\\]\\n\\nThe total number of students is:\\n\\\\[ 3k + 4k = 7k \\\\]\\n\\nThe mean of the scores of all the students is then calculated by dividing the total score by the total number of students:\\n\\\\[ \\\\frac{532k}{7k} = \\\\frac{532}{7} = 76 \\\\]\\n\\nThus, the mean of the scores of all the students is \\\\(\\\\boxed{C}\\\\).\\n\\n<|end_of_solution|>\"}],\n",
              " 'messages': [{'content': \"Return your final response within \\\\boxed{}. Ms. Blackwell gives an exam to two classes. The mean of the scores of the students in the morning class is $84$, and the afternoon class's mean score is $70$. The ratio of the number of students in the morning class to the number of students in the afternoon class is $\\\\frac{3}{4}$. What is the mean of the scores of all the students?\\n$\\\\textbf{(A)} ~74 \\\\qquad\\\\textbf{(B)} ~75 \\\\qquad\\\\textbf{(C)} ~76 \\\\qquad\\\\textbf{(D)} ~77 \\\\qquad\\\\textbf{(E)} ~78$\",\n",
              "   'role': 'user'},\n",
              "  {'content': \"<|begin_of_thought|>\\n\\nOkay, let's see. I need to find the mean score of all the students combined from both the morning and afternoon classes. Hmm, the problem gives me the means of each class and the ratio of the number of students. Let me jot down the details:\\n\\n- Morning class mean: 84\\n- Afternoon class mean: 70\\n- Ratio of students (Morning:Afternoon): 3:4\\n\\nSo, I remember that the overall mean is calculated by the total sum of all scores divided by the total number of students. Right. So, if I can find the total sum from both classes and divide by the total number of students, that should give me the mean.\\n\\nBut since I don't know the exact number of students, just the ratio, maybe I can use variables to represent them. Let's let the number of students in the morning class be 3k and the afternoon class be 4k, where k is some positive integer. That way, the ratio is maintained as 3:4. Good idea.\\n\\nNow, the total sum for the morning class would be the mean multiplied by the number of students. So, that's 84 * 3k. Similarly, the total sum for the afternoon class is 70 * 4k. Let me calculate those:\\n\\nMorning total = 84 * 3k = 252k\\n\\nAfternoon total = 70 * 4k = 280k\\n\\nSo, combined total score is 252k + 280k = 532k\\n\\nTotal number of students is 3k + 4k = 7k\\n\\nTherefore, the overall mean is total score divided by total students: 532k / 7k\\n\\nWait, the k cancels out here. So, 532 / 7. Let me compute that.\\n\\nDividing 532 by 7. Let's see, 7*70=490. 532 - 490 = 42. Then 42/7=6. So total is 70 + 6 = 76. So, 76 is the mean.\\n\\nLooking back at the options, choice C is 76. So, that's the answer. Let me just verify once again to make sure I didn't make any calculation mistakes.\\n\\nMean of morning: 84, students 3k. Total morning: 84*3k=252k. Correct.\\n\\nMean of afternoon:70, students 4k. Total afternoon:70*4k=280k. Correct.\\n\\nTotal combined: 252k + 280k = 532k. Total students:7k. 532k /7k = 76. Yep, that's right. The ratio was 3:4, so using 3k and 4k works because k cancels. So regardless of the actual number of students, as long as the ratio is 3:4, the mean will be 76. That makes sense.\\n\\nSo, the answer is C) 76.\\n\\n**Final Answer**\\n\\\\boxed{C}\\n\\n<|end_of_thought|>\\n\\n<|begin_of_solution|>\\n\\nTo find the mean of the scores of all the students combined from both the morning and afternoon classes, we start by noting the given information:\\n\\n- The mean score of the morning class is 84.\\n- The mean score of the afternoon class is 70.\\n- The ratio of the number of students in the morning class to the afternoon class is \\\\( \\\\frac{3}{4} \\\\).\\n\\nWe represent the number of students in the morning class as \\\\( 3k \\\\) and the number of students in the afternoon class as \\\\( 4k \\\\), where \\\\( k \\\\) is a positive integer. \\n\\nNext, we calculate the total sum of scores for each class:\\n- Total sum for the morning class: \\\\( 84 \\\\times 3k = 252k \\\\)\\n- Total sum for the afternoon class: \\\\( 70 \\\\times 4k = 280k \\\\)\\n\\nThe combined total score of all students is:\\n\\\\[ 252k + 280k = 532k \\\\]\\n\\nThe total number of students is:\\n\\\\[ 3k + 4k = 7k \\\\]\\n\\nThe mean of the scores of all the students is then calculated by dividing the total score by the total number of students:\\n\\\\[ \\\\frac{532k}{7k} = \\\\frac{532}{7} = 76 \\\\]\\n\\nThus, the mean of the scores of all the students is \\\\(\\\\boxed{C}\\\\).\\n\\n<|end_of_solution|>\",\n",
              "   'role': 'assistant'}]}"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset_sft[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "4NsAocMcVgrn"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Converting train dataset to ChatML: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16610/16610 [00:01<00:00, 9403.49 examples/s]\n",
            "Tokenizing train dataset:  19%|‚ñà‚ñä        | 3094/16610 [00:25<01:50, 122.32 examples/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      2\u001b[39m model_sft = AutoModelForCausalLM.from_pretrained(\n\u001b[32m      3\u001b[39m     MODEL_NAME,\n\u001b[32m      4\u001b[39m     trust_remote_code=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m      5\u001b[39m     torch_dtype=torch.bfloat16\n\u001b[32m      6\u001b[39m )\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Initialize the SFT Trainer\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m sft_trainer = \u001b[43mSFTTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_sft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                     \u001b[49m\u001b[38;5;66;43;03m# Our initialized Qwen model\u001b[39;49;00m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset_sft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m           \u001b[49m\u001b[38;5;66;43;03m# Bespoke-Stratos-17k dataset\u001b[39;49;00m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                 \u001b[49m\u001b[38;5;66;43;03m# Tokenizer\u001b[39;49;00m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                  \u001b[49m\u001b[38;5;66;43;03m# Training arguments\u001b[39;49;00m\n\u001b[32m     14\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Start the SFT Training Loop\u001b[39;00m\n\u001b[32m     17\u001b[39m sft_train_result = sft_trainer.train()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/workspace/DeepSeekR1Deployment/.venv/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:473\u001b[39m, in \u001b[36mSFTTrainer.__init__\u001b[39m\u001b[34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, compute_loss_func, compute_metrics, callbacks, optimizers, optimizer_cls_and_kwargs, preprocess_logits_for_metrics, peft_config, formatting_func)\u001b[39m\n\u001b[32m    466\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.completion_only_loss \u001b[38;5;129;01mand\u001b[39;00m formatting_func:\n\u001b[32m    467\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    468\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mA formatting function was provided while `completion_only_loss=True`, which is incompatible. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    469\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUsing a formatter converts the dataset to a language modeling type, conflicting with \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    470\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcompletion-only loss. To resolve this, apply your formatting function before passing the \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    471\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mdataset, or disable `completion_only_loss` in `SFTConfig`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    472\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m473\u001b[39m train_dataset = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prepare_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpacking\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatting_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m    475\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m eval_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    477\u001b[39m     packing = args.packing \u001b[38;5;28;01mif\u001b[39;00m args.eval_packing \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m args.eval_packing\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/workspace/DeepSeekR1Deployment/.venv/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:770\u001b[39m, in \u001b[36mSFTTrainer._prepare_dataset\u001b[39m\u001b[34m(self, dataset, processing_class, args, packing, formatting_func, dataset_name)\u001b[39m\n\u001b[32m    767\u001b[39m                 processed = {\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m: processing_class(text=example[dataset_text_field]).input_ids}\n\u001b[32m    768\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m processed\n\u001b[32m--> \u001b[39m\u001b[32m770\u001b[39m     dataset = \u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    771\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtokenize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    772\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    773\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprocessing_class\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    774\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdataset_text_field\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdataset_text_field\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    775\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43massistant_only_loss\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43massistant_only_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    776\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    777\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmap_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    778\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    780\u001b[39m \u001b[38;5;66;03m# Pack or truncate\u001b[39;00m\n\u001b[32m    781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m packing:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/workspace/DeepSeekR1Deployment/.venv/lib/python3.11/site-packages/datasets/arrow_dataset.py:557\u001b[39m, in \u001b[36mtransmit_format.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    550\u001b[39m self_format = {\n\u001b[32m    551\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_type,\n\u001b[32m    552\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mformat_kwargs\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_kwargs,\n\u001b[32m    553\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_columns,\n\u001b[32m    554\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33moutput_all_columns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._output_all_columns,\n\u001b[32m    555\u001b[39m }\n\u001b[32m    556\u001b[39m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m557\u001b[39m out: Union[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mDatasetDict\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    558\u001b[39m datasets: \u001b[38;5;28mlist\u001b[39m[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mlist\u001b[39m(out.values()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[32m    559\u001b[39m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/workspace/DeepSeekR1Deployment/.venv/lib/python3.11/site-packages/datasets/arrow_dataset.py:3079\u001b[39m, in \u001b[36mDataset.map\u001b[39m\u001b[34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc, try_original_type)\u001b[39m\n\u001b[32m   3073\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3074\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[32m   3075\u001b[39m         unit=\u001b[33m\"\u001b[39m\u001b[33m examples\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   3076\u001b[39m         total=pbar_total,\n\u001b[32m   3077\u001b[39m         desc=desc \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mMap\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   3078\u001b[39m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[32m-> \u001b[39m\u001b[32m3079\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mDataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdataset_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3080\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3081\u001b[39m \u001b[43m                \u001b[49m\u001b[43mshards_done\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/workspace/DeepSeekR1Deployment/.venv/lib/python3.11/site-packages/datasets/arrow_dataset.py:3501\u001b[39m, in \u001b[36mDataset._map_single\u001b[39m\u001b[34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, try_original_type)\u001b[39m\n\u001b[32m   3499\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m batched:\n\u001b[32m   3500\u001b[39m     _time = time.time()\n\u001b[32m-> \u001b[39m\u001b[32m3501\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miter_outputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshard_iterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3502\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mupdate_data\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3503\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m:\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/workspace/DeepSeekR1Deployment/.venv/lib/python3.11/site-packages/datasets/arrow_dataset.py:3475\u001b[39m, in \u001b[36mDataset._map_single.<locals>.iter_outputs\u001b[39m\u001b[34m(shard_iterable)\u001b[39m\n\u001b[32m   3473\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3474\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, example \u001b[38;5;129;01min\u001b[39;00m shard_iterable:\n\u001b[32m-> \u001b[39m\u001b[32m3475\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m i, \u001b[43mapply_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffset\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/workspace/DeepSeekR1Deployment/.venv/lib/python3.11/site-packages/datasets/arrow_dataset.py:3398\u001b[39m, in \u001b[36mDataset._map_single.<locals>.apply_function\u001b[39m\u001b[34m(pa_inputs, indices, offset)\u001b[39m\n\u001b[32m   3396\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Utility to apply the function on a selection of columns.\"\"\"\u001b[39;00m\n\u001b[32m   3397\u001b[39m inputs, fn_args, additional_args, fn_kwargs = prepare_inputs(pa_inputs, indices, offset=offset)\n\u001b[32m-> \u001b[39m\u001b[32m3398\u001b[39m processed_inputs = \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3399\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m prepare_outputs(pa_inputs, inputs, processed_inputs)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/workspace/DeepSeekR1Deployment/.venv/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:750\u001b[39m, in \u001b[36mSFTTrainer._prepare_dataset.<locals>.tokenize\u001b[39m\u001b[34m(example, processing_class, dataset_text_field, assistant_only_loss)\u001b[39m\n\u001b[32m    748\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# language modeling case\u001b[39;00m\n\u001b[32m    749\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_conversational(example):\n\u001b[32m--> \u001b[39m\u001b[32m750\u001b[39m         processed = \u001b[43mprocessing_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply_chat_template\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    751\u001b[39m \u001b[43m            \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    752\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    753\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreturn_assistant_tokens_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43massistant_only_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    754\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    755\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mchat_template_kwargs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    756\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    757\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33massistant_masks\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m processed \u001b[38;5;129;01mand\u001b[39;00m \u001b[32m1\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m processed[\u001b[33m\"\u001b[39m\u001b[33massistant_masks\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    758\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    759\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mYou\u001b[39m\u001b[33m'\u001b[39m\u001b[33mre using `assistant_only_loss=True`, but at least one example has no \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    760\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33massistant tokens. This usually means the tokenizer\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms chat template doesn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    763\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmasking.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    764\u001b[39m             )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/workspace/DeepSeekR1Deployment/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1667\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.apply_chat_template\u001b[39m\u001b[34m(self, conversation, tools, documents, chat_template, add_generation_prompt, continue_final_message, tokenize, padding, truncation, max_length, return_tensors, return_dict, return_assistant_tokens_mask, tokenizer_kwargs, **kwargs)\u001b[39m\n\u001b[32m   1664\u001b[39m     rendered_chat = rendered_chat[\u001b[32m0\u001b[39m]\n\u001b[32m   1666\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tokenize:\n\u001b[32m-> \u001b[39m\u001b[32m1667\u001b[39m     out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m   1668\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrendered_chat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1669\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1670\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1671\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1672\u001b[39m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1673\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1674\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1675\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1676\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m return_dict:\n\u001b[32m   1677\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m return_assistant_tokens_mask:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/workspace/DeepSeekR1Deployment/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2867\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.__call__\u001b[39m\u001b[34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[39m\n\u001b[32m   2865\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._in_target_context_manager:\n\u001b[32m   2866\u001b[39m         \u001b[38;5;28mself\u001b[39m._switch_to_input_mode()\n\u001b[32m-> \u001b[39m\u001b[32m2867\u001b[39m     encodings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2868\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2869\u001b[39m     \u001b[38;5;28mself\u001b[39m._switch_to_target_mode()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/workspace/DeepSeekR1Deployment/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2977\u001b[39m, in \u001b[36mPreTrainedTokenizerBase._call_one\u001b[39m\u001b[34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[39m\n\u001b[32m   2955\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.batch_encode_plus(\n\u001b[32m   2956\u001b[39m         batch_text_or_text_pairs=batch_text_or_text_pairs,\n\u001b[32m   2957\u001b[39m         add_special_tokens=add_special_tokens,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2974\u001b[39m         **kwargs,\n\u001b[32m   2975\u001b[39m     )\n\u001b[32m   2976\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2977\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2978\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2979\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2980\u001b[39m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2981\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2982\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2983\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2984\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2985\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2986\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2987\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2988\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2989\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2990\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2991\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2992\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2993\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2994\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2995\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2996\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2997\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2998\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/workspace/DeepSeekR1Deployment/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3052\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.encode_plus\u001b[39m\u001b[34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[39m\n\u001b[32m   3023\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3024\u001b[39m \u001b[33;03mTokenize and prepare for the model a sequence or a pair of sequences.\u001b[39;00m\n\u001b[32m   3025\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   3040\u001b[39m \u001b[33;03m        method).\u001b[39;00m\n\u001b[32m   3041\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3043\u001b[39m padding_strategy, truncation_strategy, max_length, kwargs = \u001b[38;5;28mself\u001b[39m._get_padding_truncation_strategies(\n\u001b[32m   3044\u001b[39m     padding=padding,\n\u001b[32m   3045\u001b[39m     truncation=truncation,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3049\u001b[39m     **kwargs,\n\u001b[32m   3050\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m3052\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3053\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3054\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3055\u001b[39m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3056\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3057\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3058\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3059\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3060\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3061\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3062\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3063\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3064\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3065\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3066\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3067\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3068\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3069\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3070\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3071\u001b[39m \u001b[43m    \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msplit_special_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3072\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3073\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/workspace/DeepSeekR1Deployment/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py:615\u001b[39m, in \u001b[36mPreTrainedTokenizerFast._encode_plus\u001b[39m\u001b[34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[39m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_encode_plus\u001b[39m(\n\u001b[32m    592\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    593\u001b[39m     text: Union[TextInput, PreTokenizedInput],\n\u001b[32m   (...)\u001b[39m\u001b[32m    612\u001b[39m     **kwargs,\n\u001b[32m    613\u001b[39m ) -> BatchEncoding:\n\u001b[32m    614\u001b[39m     batched_input = [(text, text_pair)] \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;28;01melse\u001b[39;00m [text]\n\u001b[32m--> \u001b[39m\u001b[32m615\u001b[39m     batched_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    616\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatched_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    617\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    618\u001b[39m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    619\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    620\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    621\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    623\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    624\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    625\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    629\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    630\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    633\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    634\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    635\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    637\u001b[39m     \u001b[38;5;66;03m# Return tensor is None, then we can remove the leading batch axis\u001b[39;00m\n\u001b[32m    638\u001b[39m     \u001b[38;5;66;03m# Overflowing tokens are returned as a batch of output so we keep them in this case\u001b[39;00m\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m return_tensors \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_overflowing_tokens:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/workspace/DeepSeekR1Deployment/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py:541\u001b[39m, in \u001b[36mPreTrainedTokenizerFast._batch_encode_plus\u001b[39m\u001b[34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens)\u001b[39m\n\u001b[32m    538\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tokenizer.encode_special_tokens != split_special_tokens:\n\u001b[32m    539\u001b[39m     \u001b[38;5;28mself\u001b[39m._tokenizer.encode_special_tokens = split_special_tokens\n\u001b[32m--> \u001b[39m\u001b[32m541\u001b[39m encodings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_tokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    542\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    543\u001b[39m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    544\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_pretokenized\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    547\u001b[39m \u001b[38;5;66;03m# Convert encoding to dict\u001b[39;00m\n\u001b[32m    548\u001b[39m \u001b[38;5;66;03m# `Tokens` has type: Tuple[\u001b[39;00m\n\u001b[32m    549\u001b[39m \u001b[38;5;66;03m#                       List[Dict[str, List[List[int]]]] or List[Dict[str, 2D-Tensor]],\u001b[39;00m\n\u001b[32m    550\u001b[39m \u001b[38;5;66;03m#                       List[EncodingFast]\u001b[39;00m\n\u001b[32m    551\u001b[39m \u001b[38;5;66;03m#                    ]\u001b[39;00m\n\u001b[32m    552\u001b[39m \u001b[38;5;66;03m# with nested dimensions corresponding to batch, overflows, sequence length\u001b[39;00m\n\u001b[32m    553\u001b[39m tokens_and_encodings = [\n\u001b[32m    554\u001b[39m     \u001b[38;5;28mself\u001b[39m._convert_encoding(\n\u001b[32m    555\u001b[39m         encoding=encoding,\n\u001b[32m   (...)\u001b[39m\u001b[32m    564\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m encoding \u001b[38;5;129;01min\u001b[39;00m encodings\n\u001b[32m    565\u001b[39m ]\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "# Initialize base model for SFT - same as before\n",
        "model_sft = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# Initialize the SFT Trainer\n",
        "sft_trainer = SFTTrainer(\n",
        "    model=model_sft,                     # Our initialized Qwen model\n",
        "    train_dataset=dataset_sft,           # Bespoke-Stratos-17k dataset\n",
        "    processing_class=tokenizer,                 # Tokenizer\n",
        "    args=training_args,                  # Training arguments\n",
        ")\n",
        "\n",
        "# Start the SFT Training Loop\n",
        "sft_train_result = sft_trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mm0IG1gsVgro"
      },
      "outputs": [],
      "source": [
        "# Saving the Trained SFT Model\n",
        "TRAINED_SFT_MODEL_PATH = \"data/Qwen-SFT-training\" # Same as OUTPUT_DIR\n",
        "\n",
        "# Save the tokenizer\n",
        "tokenizer.save_pretrained(TRAINED_SFT_MODEL_PATH)\n",
        "\n",
        "# Save the trained model\n",
        "sft_trainer.save_model(TRAINED_SFT_MODEL_PATH)\n",
        "\n",
        "print(f\"SFT Trained model saved to {TRAINED_SFT_MODEL_PATH}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
