{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "lQ4xC-yVVgqw"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import basic libraries \n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import math\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Optional\n",
    "from dotenv import load_dotenv\n",
    "import numpy as np\n",
    "\n",
    "# Import PyTorch and Hugging Face Transformers\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    "    TrainerCallback,\n",
    "    TrainerControl,\n",
    "    TrainerState,\n",
    ")\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "\n",
    "import wandb\n",
    "\n",
    "# Import dataset utilities\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Import libraries from TRL (Transformers Reinforcement Learning)\n",
    "from trl import (\n",
    "    AutoModelForCausalLMWithValueHead,\n",
    "    PPOConfig,\n",
    "    PPOTrainer,\n",
    "    GRPOTrainer,\n",
    "    GRPOConfig,\n",
    "    SFTTrainer\n",
    ")\n",
    "\n",
    "# Import math-related utilities\n",
    "from latex2sympy2_extended import NormalizationConfig\n",
    "from math_verify import LatexExtractionConfig, parse, verify\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"WANDB_API_KEY\")\n",
    "wandb.login(key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ MacBook mode: Using FP32 (no mixed precision)\n",
      "Torch dtype: float32\n",
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# CONFIGURATION VARIABLES\n",
    "# ================================\n",
    "\n",
    "# Which between the two setups\n",
    "on_macbook = True\n",
    "\n",
    "# Simple toggle: Set to True for CUDA GPU with bf16 support, False for MacBook\n",
    "if on_macbook:\n",
    "    USE_CUDA_BF16 = False  # Change to True when using CUDA GPU with bf16 support\n",
    "else:\n",
    "    USE_CUDA_BF16 = True\n",
    "\n",
    "if USE_CUDA_BF16:\n",
    "    # CUDA GPU with bf16 support\n",
    "    TORCH_DTYPE = torch.bfloat16\n",
    "    TORCH_DTYPE_STR = \"bfloat16\"\n",
    "    USE_BF16 = True\n",
    "    USE_FP16 = False\n",
    "    print(\"✓ CUDA GPU mode: Using BF16 mixed precision\")\n",
    "else:\n",
    "    # MacBook mode (MPS/CPU)\n",
    "    TORCH_DTYPE = torch.float32\n",
    "    TORCH_DTYPE_STR = \"float32\"\n",
    "    USE_BF16 = False\n",
    "    USE_FP16 = False\n",
    "    print(\"✓ MacBook mode: Using FP32 (no mixed precision)\")\n",
    "\n",
    "if on_macbook:\n",
    "    batch_size = 4\n",
    "    gradient_acc = 2\n",
    "else:\n",
    "    batch_size = 8\n",
    "    gradient_acc = 2\n",
    "\n",
    "print(f\"Torch dtype: {TORCH_DTYPE_STR}\")\n",
    "\n",
    "# Check device availability and set device accordingly\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nm1AHSltVgq4",
    "outputId": "46c12947-6912-49ef-c92f-617073ef013d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'system': \"Your role as an assistant involves thoroughly exploring questions through a systematic long thinking process before providing the final precise and accurate solutions. This requires engaging in a comprehensive cycle of analysis, summarizing, exploration, reassessment, reflection, backtracing, and iteration to develop well-considered thinking process. Please structure your response into two main sections: Thought and Solution. In the Thought section, detail your reasoning process using the specified format: <|begin_of_thought|> {thought with steps separated with '\\\\n\\\\n'} <|end_of_thought|> Each step should include detailed considerations such as analisying questions, summarizing relevant findings, brainstorming new ideas, verifying the accuracy of the current steps, refining any errors, and revisiting previous steps. In the Solution section, based on various attempts, explorations, and reflections from the Thought section, systematically present the final solution that you deem correct. The solution should remain a logical, accurate, concise expression style and detail necessary step needed to reach the conclusion, formatted as follows: <|begin_of_solution|> {final formatted, precise, and clear solution} <|end_of_solution|> Now, try to solve the following question through the above guidelines:\",\n",
       " 'conversations': [{'from': 'user',\n",
       "   'value': 'Return your final response within \\\\boxed{}. The operation $\\\\otimes$ is defined for all nonzero numbers by $a\\\\otimes b =\\\\frac{a^{2}}{b}$. Determine $[(1\\\\otimes 2)\\\\otimes 3]-[1\\\\otimes (2\\\\otimes 3)]$.\\n$\\\\text{(A)}\\\\ -\\\\frac{2}{3}\\\\qquad\\\\text{(B)}\\\\ -\\\\frac{1}{4}\\\\qquad\\\\text{(C)}\\\\ 0\\\\qquad\\\\text{(D)}\\\\ \\\\frac{1}{4}\\\\qquad\\\\text{(E)}\\\\ \\\\frac{2}{3}$'},\n",
       "  {'from': 'assistant',\n",
       "   'value': \"<|begin_of_thought|>\\n\\nOkay, let me try to figure out this problem. So, we have this operation defined as a⊗b = a²/b. And we need to compute [(1⊗2)⊗3] - [1⊗(2⊗3)]. Then choose the correct answer from the options given. Alright, let's break it down step by step.\\n\\nFirst, I need to remember that the operation ⊗ is not associative, right? Because the problem is asking for the difference between two different groupings: (1⊗2)⊗3 and 1⊗(2⊗3). So, the order in which we perform the operations matters here. That's probably why there's a subtraction between them.\\n\\nLet me start by computing each part separately. Let's tackle the first part: (1⊗2)⊗3.\\n\\nStarting with the innermost operation, which is 1⊗2. According to the definition, a⊗b = a²/b. So here, a is 1 and b is 2. Plugging those in: 1² / 2 = 1/2. So, 1⊗2 equals 1/2.\\n\\nNow, we take that result and perform the next operation with 3. So, (1⊗2)⊗3 becomes (1/2)⊗3. Again, using the same definition: a is now 1/2 and b is 3. So, ( (1/2)² ) / 3 = (1/4) / 3 = 1/12. So, (1⊗2)⊗3 equals 1/12.\\n\\nAlright, that's the first part. Now let's compute the second part: 1⊗(2⊗3). Again, starting with the innermost operation, which is 2⊗3. Applying the definition: a is 2 and b is 3. So, 2² / 3 = 4/3. Therefore, 2⊗3 equals 4/3.\\n\\nNow, we need to compute 1⊗(4/3). Here, a is 1 and b is 4/3. Using the operation definition: 1² / (4/3) = 1 / (4/3) = 3/4. So, 1⊗(2⊗3) equals 3/4.\\n\\nNow, the problem asks for the difference between the two results: [(1⊗2)⊗3] - [1⊗(2⊗3)] = (1/12) - (3/4). To subtract these fractions, they need a common denominator. The denominators are 12 and 4, so 12 is the common denominator.\\n\\nConverting 3/4 to twelfths: 3/4 = 9/12. So, 1/12 - 9/12 = (1 - 9)/12 = -8/12. Simplifying that fraction by dividing numerator and denominator by 4: -8/12 = -2/3.\\n\\nHmm, looking at the answer choices, option A is -2/3. So, is that the answer? Wait, but let me double-check my calculations to make sure I didn't make a mistake somewhere.\\n\\nFirst, checking (1⊗2): 1² / 2 = 1/2. Correct. Then, (1/2)⊗3: (1/2)² / 3 = (1/4)/3 = 1/12. That seems right.\\n\\nNow, for 2⊗3: 2² / 3 = 4/3. Correct. Then, 1⊗(4/3): 1² / (4/3) = 1 / (4/3) = 3/4. Yes, that's correct.\\n\\nSubtracting 3/4 from 1/12: 1/12 - 3/4. Convert 3/4 to 9/12, so 1/12 - 9/12 = -8/12 = -2/3. Yes, that all checks out. So the answer should be -2/3, which is option A.\\n\\nWait, but let me think again. The operation is defined for all nonzero numbers, so we don't have any issues with division by zero here. 2⊗3 is 4/3, which is fine, and then 1⊗(4/3) is 3/4. Correct.\\n\\nAlternatively, maybe there's a different way to approach the problem? Let me try expanding both expressions using variables to see if there's a pattern.\\n\\nLet’s denote the first expression: (a⊗b)⊗c. Using the definition:\\n\\nFirst, compute a⊗b = a²/b.\\n\\nThen, take that result and ⊗ with c: (a²/b)⊗c = ( (a²/b)² ) / c = a⁴ / (b² c).\\n\\nNow, the second expression: a⊗(b⊗c). First compute b⊗c = b²/c.\\n\\nThen, a⊗(b²/c) = a² / (b²/c) = a² * (c / b²) = (a² c) / b².\\n\\nTherefore, the difference between the two expressions is:\\n\\n(a⁴ / (b² c)) - (a² c / b²) = (a⁴ - a² c²) / (b² c) = a² (a² - c²) / (b² c).\\n\\nHmm, factoring that, it's a² (a - c)(a + c) / (b² c).\\n\\nBut in our specific problem, a = 1, b = 2, c = 3. Plugging those values in:\\n\\n1² (1 - 3)(1 + 3) / (2² * 3) = 1 * (-2)(4) / (4 * 3) = (-8) / 12 = -2/3. Same result. So that confirms the answer is indeed -2/3.\\n\\nTherefore, I think my initial calculation was correct, and the answer is option A.\\n\\n**Final Answer**\\n\\\\boxed{A}\\n\\n<|end_of_thought|>\\n\\n<|begin_of_solution|>\\n\\nTo determine the value of \\\\([(1 \\\\otimes 2) \\\\otimes 3] - [1 \\\\otimes (2 \\\\otimes 3)]\\\\) where the operation \\\\(\\\\otimes\\\\) is defined by \\\\(a \\\\otimes b = \\\\frac{a^2}{b}\\\\), we proceed as follows:\\n\\nFirst, compute \\\\(1 \\\\otimes 2\\\\):\\n\\\\[\\n1 \\\\otimes 2 = \\\\frac{1^2}{2} = \\\\frac{1}{2}\\n\\\\]\\nNext, use this result to compute \\\\((1 \\\\otimes 2) \\\\otimes 3\\\\):\\n\\\\[\\n\\\\left(\\\\frac{1}{2}\\\\right) \\\\otimes 3 = \\\\frac{\\\\left(\\\\frac{1}{2}\\\\right)^2}{3} = \\\\frac{\\\\frac{1}{4}}{3} = \\\\frac{1}{12}\\n\\\\]\\n\\nNow, compute \\\\(2 \\\\otimes 3\\\\):\\n\\\\[\\n2 \\\\otimes 3 = \\\\frac{2^2}{3} = \\\\frac{4}{3}\\n\\\\]\\nThen, use this result to compute \\\\(1 \\\\otimes (2 \\\\otimes 3)\\\\):\\n\\\\[\\n1 \\\\otimes \\\\left(\\\\frac{4}{3}\\\\right) = \\\\frac{1^2}{\\\\frac{4}{3}} = \\\\frac{1}{\\\\frac{4}{3}} = \\\\frac{3}{4}\\n\\\\]\\n\\nFinally, find the difference between the two results:\\n\\\\[\\n\\\\frac{1}{12} - \\\\frac{3}{4} = \\\\frac{1}{12} - \\\\frac{9}{12} = \\\\frac{1 - 9}{12} = \\\\frac{-8}{12} = -\\\\frac{2}{3}\\n\\\\]\\n\\nThus, the answer is \\\\(\\\\boxed{A}\\\\).\\n\\n<|end_of_solution|>\"}]}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the \"AI-MO/NuminaMath-TIR\" dataset from DigitalLearningGmbH\n",
    "MATH_le = load_dataset(\"AI-MO/NuminaMath-TIR\", \"default\")\n",
    "\n",
    "# Access the first sample in the training set\n",
    "MATH_le['train'][0]\n",
    "\n",
    "# Load the \"Bespoke-Stratos-17k\" dataset from bespokelabs\n",
    "bespoke_rl = load_dataset(\"bespokelabs/Bespoke-Stratos-17k\", \"default\")\n",
    "\n",
    "# Access the first sample in the training set\n",
    "bespoke_rl['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "grmQEuCxVgrB",
    "outputId": "2ecf6108-f018-4c05-b823-e907f91ad067"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 151665\n",
      "Model max length: 131072\n",
      "Pad token: <|endoftext|>\n",
      "EOS token: <|im_end|>\n",
      "Model parameters: 494,032,768\n"
     ]
    }
   ],
   "source": [
    "# MODEL_NAME = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-0.5B-Instruct\"  # Use Qwen model which has chat template support\n",
    "# OUTPUT_DIR = \"data/Qwen-GRPO-training\" # For saving our trained model\n",
    "OUTPUT_DIR = \"data/Qwen-GRPO-training\"  # For saving our trained model\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Initialize tokenizer with chat template\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    trust_remote_code=True,\n",
    "    padding_side=\"right\"\n",
    ")\n",
    "\n",
    "# Set pad token if not set\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Vocabulary size: {len(tokenizer)}\")\n",
    "print(f\"Model max length: {tokenizer.model_max_length}\")\n",
    "print(f\"Pad token: {tokenizer.pad_token}\")\n",
    "print(f\"EOS token: {tokenizer.eos_token}\")\n",
    "\n",
    "# Initialize base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=TORCH_DTYPE  # Use configuration variable for dtype\n",
    ")\n",
    "\n",
    "print(f\"Model parameters: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rGWn0oYDVgrE",
    "outputId": "3e10074e-ab9a-45a4-a0bf-5e895e64ceaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Input: how are you?\n",
      "Model Response: system\n",
      "You are Qwen, a helpful assistant.\n",
      "user\n",
      "how are you?\n",
      "assistant\n",
      "Hello! I'm just a large language model created by Alibaba Cloud. I don't have feelings or emotions in the traditional sense. However, I'm here to assist you with any questions or tasks you might have. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "# Move model to the appropriate device\n",
    "model.to(device)\n",
    "\n",
    "# Test basic inference\n",
    "def test_model_inference(user_input: str):\n",
    "    \"\"\"Test basic model inference with the loaded model and tokenizer.\"\"\"\n",
    "    \n",
    "    # Check if the tokenizer has a chat template\n",
    "    if hasattr(tokenizer, 'chat_template') and tokenizer.chat_template is not None:\n",
    "        # Use chat template if available (for Qwen models)\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are Qwen, a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": user_input}\n",
    "        ]\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "    else:\n",
    "        # Fallback for models without chat templates (like tiny-gpt2)\n",
    "        text = f\"Question: {user_input}\\nAnswer:\"\n",
    "\n",
    "    # Tokenize and generate\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        pad_token_id=tokenizer.eos_token_id  # Ensure proper padding\n",
    "    )\n",
    "\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Test the model\n",
    "test_input = \"how are you?\"\n",
    "response = test_model_inference(test_input)\n",
    "print(f\"Test Input: {test_input}\")\n",
    "print(f\"Model Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "DNsOH2URVgrG"
   },
   "outputs": [],
   "source": [
    "# DeepSeek system prompt for GRPO based training\n",
    "SYSTEM_PROMPT = (\n",
    "    \"A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant \"\n",
    "    \"first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning \"\n",
    "    \"process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., \"\n",
    "    \"<think> reasoning process here </think><answer> answer here </answer>\"\n",
    ")\n",
    "\n",
    "log_dict = {\n",
    "    \"train_loss\": [],\n",
    "    \"learning_rate\": [],\n",
    "    \"accuracy_reward\": [],\n",
    "    \"format_reward\": [],\n",
    "    \"reasoning_steps_reward\": [],\n",
    "    \"cosine_scaled_reward\": [],\n",
    "    \"repetition_penalty_reward\": [],\n",
    "    \"total_reward\": [],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "O7pJISXpVgrH"
   },
   "outputs": [],
   "source": [
    "# Function to structure the training data\n",
    "def make_conversation(example):\n",
    "    \"\"\"Convert dataset examples into conversation format.\"\"\"\n",
    "    return {\n",
    "        \"prompt\": [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": example[\"problem\"]},\n",
    "        ],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "7GhKxEqqVgrI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 72441\n",
      "Test set size: 99\n"
     ]
    }
   ],
   "source": [
    "# Load and prepare dataset\n",
    "def load_math_dataset():\n",
    "    \"\"\"Load and prepare the mathematics dataset.\"\"\"\n",
    "    dataset = load_dataset(\n",
    "        \"AI-MO/NuminaMath-TIR\",\n",
    "        name=\"default\",\n",
    "        split=['train', 'test']\n",
    "    )\n",
    "\n",
    "    # Convert splits into dictionary\n",
    "    dataset = {\n",
    "        'train': dataset[0],\n",
    "        'test': dataset[1]\n",
    "    }\n",
    "\n",
    "    # Apply conversation format\n",
    "    for split in dataset:\n",
    "        dataset[split] = dataset[split].map(make_conversation)\n",
    "\n",
    "        # Remove 'messages' column if exists\n",
    "        if \"messages\" in dataset[split].column_names:\n",
    "            dataset[split] = dataset[split].remove_columns(\"messages\")\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# Load our training dataset and printing train/test size\n",
    "dataset = load_math_dataset()\n",
    "\n",
    "print(f\"Train set size: {len(dataset['train'])}\")\n",
    "print(f\"Test set size: {len(dataset['test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jdAvBnP-VgrK",
    "outputId": "bae213be-6234-4246-c3be-e8cbd3d16c67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validating train split:\n",
      "✓ All required fields present\n",
      "✓ Prompt format is correct\n",
      "\n",
      "Validating test split:\n",
      "✓ All required fields present\n",
      "✓ Prompt format is correct\n"
     ]
    }
   ],
   "source": [
    "def validate_dataset(dataset):\n",
    "    \"\"\"Perform basic validation checks on the dataset.\"\"\"\n",
    "\n",
    "    # Define the required fields for the dataset\n",
    "    required_fields = [\"problem\", \"prompt\"]\n",
    "\n",
    "    # Loop through the 'train' and 'test' splits of the dataset\n",
    "    for split in ['train', 'test']:\n",
    "        print(f\"\\nValidating {split} split:\")\n",
    "\n",
    "        # Retrieve column names from the dataset\n",
    "        fields = dataset[split].column_names\n",
    "\n",
    "        # Check if any required fields are missing\n",
    "        missing = [field for field in required_fields if field not in fields]\n",
    "        if missing:\n",
    "            print(f\"Warning: Missing fields: {missing}\")  # Warn if fields are missing\n",
    "        else:\n",
    "            print(\"✓ All required fields present\")  # Confirm all fields are present\n",
    "\n",
    "        # Retrieve the first sample from the dataset split\n",
    "        sample = dataset[split][0]\n",
    "\n",
    "        # Extract the 'prompt' field, which contains a list of messages\n",
    "        messages = sample['prompt']\n",
    "\n",
    "        # Validate the prompt format:\n",
    "        # - It should contain at least two messages\n",
    "        # - The first message should be from the 'system' role\n",
    "        # - The second message should be from the 'user' role\n",
    "        if (len(messages) >= 2 and\n",
    "            messages[0]['role'] == 'system' and\n",
    "            messages[1]['role'] == 'user'):\n",
    "            print(\"✓ Prompt format is correct\")  # Confirm correct format\n",
    "        else:\n",
    "            print(\"Warning: Incorrect prompt format\")  # Warn if format is incorrect\n",
    "\n",
    "# Validate dataset\n",
    "validate_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "xUbz3WIiVgrL"
   },
   "outputs": [],
   "source": [
    "def accuracy_reward(completions, **kwargs):\n",
    "    \"\"\"\n",
    "    Reward function to check if the model's response is mathematically\n",
    "    equivalent to the ground truth solution.\n",
    "    Uses latex2sympy2 for parsing and math_verify for validation.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract responses\n",
    "    contents = [completion[0][\"content\"] for completion in completions]\n",
    "    rewards = []\n",
    "\n",
    "    solutions = kwargs.get(\"solution\") # Get solutions from kwargs\n",
    "\n",
    "    for content, sol in zip(contents, solutions):\n",
    "        # Parse the ground truth solution\n",
    "        gold_parsed = parse(sol, extraction_mode=\"first_match\",\n",
    "                            extraction_config=[LatexExtractionConfig()])\n",
    "\n",
    "        if gold_parsed:  # Check if parsing was successful\n",
    "            # Parse the model's answer with relaxed normalization\n",
    "            answer_parsed = parse(\n",
    "                content,\n",
    "                extraction_config=[\n",
    "                    LatexExtractionConfig(\n",
    "                        normalization_config=NormalizationConfig(\n",
    "                            nits=False,\n",
    "                            malformed_operators=False,\n",
    "                            basic_latex=True,\n",
    "                            # equations=True, # equations argument is deprecated\n",
    "                            boxed=\"all\",\n",
    "                            units=True,\n",
    "                        ),\n",
    "                        boxed_match_priority=0,\n",
    "                        try_extract_without_anchor=False,\n",
    "                    )\n",
    "                ],\n",
    "                extraction_mode=\"first_match\",\n",
    "            )\n",
    "\n",
    "            # Reward 1.0 if correct, 0.0 if incorrect\n",
    "            reward = float(verify(answer_parsed, gold_parsed))\n",
    "        else:\n",
    "            # If ground truth cannot be parsed, assign neutral reward (0.5)\n",
    "            reward = 0.5\n",
    "            print(\"Warning: Failed to parse gold solution:\", sol)\n",
    "\n",
    "        rewards.append(reward)\n",
    "\n",
    "    # Log the rewards in log dict\n",
    "    log_dict[\"accuracy_reward\"].extend(rewards)\n",
    "\n",
    "    return rewards\n",
    "\n",
    "# Implement Format Reward Function\n",
    "def format_reward(completions, **kwargs):\n",
    "    \"\"\"\n",
    "    Reward function to check if the completion has the correct format:\n",
    "    <think>...</think> <answer>...</answer>.\n",
    "    \"\"\"\n",
    "    # Define the regex pattern for the desired format\n",
    "    pattern = r\"^<think>.*?</think>\\s*<answer>.*?</answer>$\"\n",
    "\n",
    "    # Extract the content from each completion\n",
    "    completion_contents = [completion[0][\"content\"] for completion in completions]\n",
    "\n",
    "    # Check if each completion matches the pattern\n",
    "    matches = [re.match(pattern, content, re.DOTALL | re.MULTILINE)\n",
    "               for content in completion_contents]\n",
    "\n",
    "    # Reward 1.0 for correct format, 0.0 otherwise\n",
    "    rewards = [1.0 if match else 0.0 for match in matches]\n",
    "\n",
    "    # Log the rewards in log dict\n",
    "    log_dict[\"format_reward\"].extend(rewards)\n",
    "\n",
    "    return rewards\n",
    "\n",
    "def reasoning_steps_reward(completions, **kwargs):\n",
    "    r\"\"\"\n",
    "    Reward function to encourage clear step-by-step reasoning.\n",
    "    It looks for patterns like \"Step 1:\", numbered lists, bullet points,\n",
    "    and transition words.\n",
    "    \"\"\"\n",
    "    # Regex pattern to find indicators of reasoning steps\n",
    "    pattern = r\"(Step \\d+:|^\\d+\\.|\\n-|\\n\\*|First,|Second,|Next,|Finally,)\"\n",
    "\n",
    "    # Extract completion contents\n",
    "    completion_contents = [completion[0][\"content\"] for completion in completions]\n",
    "\n",
    "    # Count the number of reasoning step indicators in each completion\n",
    "    matches = [len(re.findall(pattern, content, re.MULTILINE))\n",
    "               for content in completion_contents]\n",
    "\n",
    "    # Reward is proportional to the number of reasoning steps, maxing out at 1.0\n",
    "    # We're using a \"magic number\" 3 here - encourage at least 3 steps for full reward\n",
    "    rewards = [min(1.0, count / 3) for count in matches]\n",
    "\n",
    "    # Log the rewards in log dict\n",
    "    log_dict[\"reasoning_steps_reward\"].extend(rewards)\n",
    "\n",
    "    return rewards\n",
    "\n",
    "# Implement Cosine Scaled Reward Function\n",
    "def get_cosine_scaled_reward(\n",
    "    min_value_wrong: float = -0.5,\n",
    "    max_value_wrong: float = -0.1,\n",
    "    min_value_correct: float = 0.8,\n",
    "    max_value_correct: float = 1.0,\n",
    "    max_len: int = 1000,\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns a cosine scaled reward function. This function scales the accuracy reward\n",
    "    based on completion length. Shorter correct solutions get higher rewards,\n",
    "    longer incorrect solutions get less penalty.\n",
    "    \"\"\"\n",
    "    def cosine_scaled_reward(completions, solution, accuracy_rewards, **kwargs):\n",
    "        \"\"\"\n",
    "        Cosine scaled reward function that adjusts accuracy rewards based on completion length.\n",
    "        \"\"\"\n",
    "        contents = [completion[0][\"content\"] for completion in completions]\n",
    "        rewards = []\n",
    "\n",
    "        for content, sol, acc_reward in zip(contents, solution, accuracy_rewards):\n",
    "            gen_len = len(content)  # Length of the generated answer\n",
    "            progress = gen_len / max_len # How far we are to max length\n",
    "            cosine = math.cos(progress * math.pi) # Cosine value based on progress\n",
    "\n",
    "            if acc_reward > 0.5: # Assuming accuracy_reward gives ~1.0 for correct answers\n",
    "                min_value = min_value_correct\n",
    "                max_value = max_value_correct\n",
    "            else: # Incorrect answer\n",
    "                min_value = max_value_wrong  # Note the swap!\n",
    "                max_value = min_value_wrong\n",
    "\n",
    "            # Cosine scaling formula!\n",
    "            reward = min_value + 0.5 * (max_value - min_value) * (1.0 + cosine)\n",
    "            rewards.append(float(reward))\n",
    "\n",
    "        # Log the rewards in log dict\n",
    "        log_dict[\"cosine_scaled_reward\"].extend(rewards)\n",
    "\n",
    "        return rewards\n",
    "    \n",
    "    return cosine_scaled_reward\n",
    "\n",
    "def get_repetition_penalty_reward(ngram_size: int = 3, max_penalty: float = -0.1):\n",
    "    \"\"\"\n",
    "    Returns a repetition penalty reward function. Penalizes repetitions of n-grams\n",
    "    in the generated text.\n",
    "    \"\"\"\n",
    "    if max_penalty > 0:\n",
    "        raise ValueError(f\"max_penalty {max_penalty} should not be positive\")\n",
    "\n",
    "    def zipngram(text: str, ngram_size: int):\n",
    "        \"\"\"Helper function to generate n-grams from text.\"\"\"\n",
    "        words = text.lower().split() # Lowercase and split into words\n",
    "        return zip(*[words[i:] for i in range(ngram_size)]) # Create n-grams\n",
    "\n",
    "    def repetition_penalty_reward(completions, **kwargs) -> float:\n",
    "        \"\"\"\n",
    "        Repetition penalty reward function.\n",
    "        \"\"\"\n",
    "        contents = [completion[0][\"content\"] for completion in completions]\n",
    "        rewards = []\n",
    "        for completion in contents:\n",
    "            if completion == \"\": # No penalty for empty completions\n",
    "                rewards.append(0.0)\n",
    "                continue\n",
    "            if len(completion.split()) < ngram_size: # No penalty for short completions\n",
    "                rewards.append(0.0)\n",
    "                continue\n",
    "\n",
    "            ngrams = set() # Use a set to store unique n-grams\n",
    "            total = 0\n",
    "            for ng in zipngram(completion, ngram_size): # Generate n-grams\n",
    "                ngrams.add(ng) # Add n-gram to the set (duplicates are ignored)\n",
    "                total += 1 # Count total n-grams\n",
    "\n",
    "            # Calculate scaling factor: more repetition -> higher scaling\n",
    "            scaling = 1 - len(ngrams) / total\n",
    "            reward = scaling * max_penalty # Apply penalty based on scaling\n",
    "            rewards.append(reward)\n",
    "\n",
    "        # Log the rewards in log dict\n",
    "        log_dict[\"repetition_penalty_reward\"].extend(rewards)\n",
    "\n",
    "        return rewards\n",
    "    \n",
    "    return repetition_penalty_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "jUOS2A-BVgrb"
   },
   "outputs": [],
   "source": [
    "# Define GRPOScriptArguments for reward function parameters\n",
    "@dataclass\n",
    "class GRPOScriptArguments:\n",
    "    \"\"\"\n",
    "    Script arguments for GRPO training, specifically related to reward functions.\n",
    "    \"\"\"\n",
    "\n",
    "    reward_funcs: list[str] = field(\n",
    "        default_factory=lambda: [\"accuracy\", \"format\"],\n",
    "        metadata={\n",
    "            \"help\": \"List of reward functions. Possible values: 'accuracy', 'format', 'reasoning_steps', 'cosine', 'repetition_penalty'\"\n",
    "        },\n",
    "    )\n",
    "    cosine_min_value_wrong: float = field(\n",
    "        default=-0.5,\n",
    "        metadata={\"help\": \"Minimum reward for cosine scaling for wrong answers\"},\n",
    "    )\n",
    "    cosine_max_value_wrong: float = field(\n",
    "        default=-0.1,\n",
    "        metadata={\"help\": \"Maximum reward for cosine scaling for wrong answers\"},\n",
    "    )\n",
    "    cosine_min_value_correct: float = field(\n",
    "        default=0.8,\n",
    "        metadata={\"help\": \"Minimum reward for cosine scaling for correct answers\"},\n",
    "    )\n",
    "    cosine_max_value_correct: float = field(\n",
    "        default=1.0,\n",
    "        metadata={\"help\": \"Maximum reward for cosine scaling for correct answers\"},\n",
    "    )\n",
    "    cosine_max_len: int = field(\n",
    "        default=1000,\n",
    "        metadata={\"help\": \"Maximum length for cosine scaling\"},\n",
    "    )\n",
    "\n",
    "    repetition_n_grams: int = field(\n",
    "        default=3,\n",
    "        metadata={\"help\": \"Number of n-grams for repetition penalty reward\"},\n",
    "    )\n",
    "    repetition_max_penalty: float = field(\n",
    "        default=-0.1,\n",
    "        metadata={\"help\": \"Maximum (negative) penalty for for repetition penalty reward\"},\n",
    "    )\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"\n",
    "    Configuration for the model.\n",
    "    \"\"\"\n",
    "    model_name_or_path: str = field(\n",
    "        default=MODEL_NAME, metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n",
    "    )\n",
    "    model_revision: Optional[str] = field(\n",
    "        default=\"main\", metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"}\n",
    "    )\n",
    "    torch_dtype: Optional[str] = field(\n",
    "        default=TORCH_DTYPE_STR, metadata={\"help\": \"Override the default `torch_dtype` and load the model under this dtype.\"}\n",
    "    )\n",
    "    trust_remote_code: bool = field(\n",
    "        default=True, metadata={\"help\": \"Trust remote code when loading model and tokenizer.\"}\n",
    "    )\n",
    "    attn_implementation: Optional[str] = field(\n",
    "        default=\"flash_attention_2\", metadata={\"help\": \"Attention implementation to use. 'flash_attention_2' or None\"}\n",
    "    )\n",
    "\n",
    "# Define TrainingArguments from transformers\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,          # Output directory for checkpoints and logs\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,             # Total number of training epochs\n",
    "    per_device_train_batch_size=batch_size,  # Batch size per device during training\n",
    "    per_device_eval_batch_size=batch_size*2,   # Batch size for evaluation\n",
    "    gradient_accumulation_steps=gradient_acc,  # Accumulate gradients to simulate larger batch size\n",
    "    learning_rate=5e-5,            # Initial learning rate for AdamW optimizer\n",
    "    warmup_ratio=0.1,              # Linear warmup over warmup_ratio fraction of training steps\n",
    "    weight_decay=0.01,             # Apply weight decay to all layers except bias and LayerNorm weights\n",
    "    logging_steps=1,              # Log every X updates steps\n",
    "    eval_strategy=\"steps\",         # Evaluate every `eval_steps`\n",
    "    eval_steps=50,                 # Evaluation and logging steps\n",
    "    save_strategy=\"steps\",         # Save checkpoint every `save_steps`\n",
    "    save_steps=50,                 # Save checkpoint every X updates steps\n",
    "    save_total_limit=2,            # Limit the total amount of checkpoints. Deletes the older checkpoints.\n",
    "    # dataloader_num_workers=2,      # Number of subprocesses to use for data loading\n",
    "    seed=42,                       # Random seed for reproducibility\n",
    "    bf16=USE_BF16,                 # Use BF16 mixed precision for CUDA\n",
    "    fp16=USE_FP16,                 # Use FP16 mixed precision (not used in this config)\n",
    "    push_to_hub=False,             # Whether to push the final model to Hugging Face Hub\n",
    "    gradient_checkpointing=True,   # Enable gradient checkpointing\n",
    "    report_to=[],                  # Reporting to no one\n",
    "    remove_unused_columns=False,   # Do not remove unused columns from the dataset\n",
    ")\n",
    "\n",
    "# Instantiate configuration objects\n",
    "script_args = GRPOScriptArguments()\n",
    "model_args = ModelConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "_Wv_6v71Vgrf"
   },
   "outputs": [],
   "source": [
    "# Utility function to get reward functions based on script arguments\n",
    "def get_reward_functions(script_args):\n",
    "    \"\"\"\n",
    "    Returns a list of reward functions based on the script arguments.\n",
    "    \"\"\"\n",
    "    reward_funcs_list = []\n",
    "    reward_funcs_registry = {\n",
    "        \"accuracy\": accuracy_reward,  # Assuming accuracy_reward is defined in previous steps\n",
    "        \"format\": format_reward,      # Assuming format_reward is defined in previous steps\n",
    "        \"reasoning_steps\": reasoning_steps_reward, # Assuming reasoning_steps_reward is defined\n",
    "        \"cosine\": get_cosine_scaled_reward( # Assuming get_cosine_scaled_reward is defined\n",
    "            min_value_wrong=script_args.cosine_min_value_wrong,\n",
    "            max_value_wrong=script_args.cosine_max_value_wrong,\n",
    "            min_value_correct=script_args.cosine_min_value_correct,\n",
    "            max_value_correct=script_args.cosine_max_value_correct,\n",
    "            max_len=script_args.cosine_max_len,\n",
    "        ),\n",
    "        \"repetition_penalty\": get_repetition_penalty_reward( # Assuming get_repetition_penalty_reward is defined\n",
    "            ngram_size=script_args.repetition_n_grams,\n",
    "            max_penalty=script_args.repetition_max_penalty,\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    for func_name in script_args.reward_funcs:\n",
    "        if func_name not in reward_funcs_registry:\n",
    "            raise ValueError(f\"Reward function '{func_name}' not found in registry.\")\n",
    "        reward_funcs_list.append(reward_funcs_registry[func_name])\n",
    "\n",
    "    return reward_funcs_list\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class LoggingCallback(TrainerCallback):\n",
    "    \"\"\"\n",
    "    A simple callback for logging training information at specific steps.\n",
    "    \"\"\"\n",
    "    def on_step_end(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n",
    "        if state.global_step % args.logging_steps == 0:\n",
    "            if state.log_history and len(state.log_history) > 0:\n",
    "                batch_size = args.per_device_train_batch_size * args.gradient_accumulation_steps\n",
    "                current_idx = (state.global_step - args.logging_steps) * batch_size\n",
    "                accuracy_reward = np.mean(log_dict[\"accuracy_reward\"][current_idx:])\n",
    "                format_reward = np.mean(log_dict[\"format_reward\"][current_idx:])\n",
    "                reasoning_steps_reward = np.mean(log_dict[\"reasoning_steps_reward\"][current_idx:])\n",
    "                cosine_scaled_reward = np.mean(log_dict[\"cosine_scaled_reward\"][current_idx:])\n",
    "                repetition_penalty_reward = np.mean(log_dict[\"repetition_penalty_reward\"][current_idx:])\n",
    "                rewards = [reward for reward in [accuracy_reward, format_reward, \\\n",
    "                                                reasoning_steps_reward, cosine_scaled_reward, \\\n",
    "                                                    repetition_penalty_reward] if not np.isnan(reward)]\n",
    "                total_reward = sum(rewards)\n",
    "                logs = {\n",
    "                    \"step\": state.global_step,\n",
    "                    \"train_loss\": state.log_history[-1][\"loss\"],\n",
    "                    \"learning_rate\": state.log_history[-1][\"learning_rate\"],\n",
    "                    \"accuracy_reward\": accuracy_reward,\n",
    "                    \"format_reward\": format_reward,\n",
    "                    \"reasoning_steps_reward\": reasoning_steps_reward,\n",
    "                    \"cosine_scaled_reward\": cosine_scaled_reward,\n",
    "                    \"repetition_penalty_reward\": repetition_penalty_reward,\n",
    "                    \"total_reward\": total_reward\n",
    "                }\n",
    "                print(f\"Step {state.global_step} - Logging: {logs}\")\n",
    "    \n",
    "                # log to wandb\n",
    "                wandb.log(logs)\n",
    "\n",
    "def get_callbacks(training_args, model_args, script_args):\n",
    "    \"\"\"\n",
    "    Returns a list of callbacks to be used during training.\n",
    "    For now, it includes only the LoggingCallback. You can extend this to add more callbacks.\n",
    "    \"\"\"\n",
    "    callbacks = [LoggingCallback()] # Instantiate our LoggingCallback\n",
    "    return callbacks\n",
    "\n",
    "# Get reward functions and callbacks\n",
    "reward_functions = get_reward_functions(script_args)\n",
    "callbacks = get_callbacks(training_args, model_args, script_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CustomGRPOTrainer(GRPOTrainer):\n",
    "#     def log(self, logs: dict, start_time: float = None):\n",
    "#         print(logs)\n",
    "\n",
    "#         if hasattr(self, \"state\"):\n",
    "#             step = self.state.global_step\n",
    "#             logs[\"step\"] = step\n",
    "#         print(logs)\n",
    "#         print(\"---------------\")\n",
    "\n",
    "#         if logs:\n",
    "#             wandb.log(logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SKDIAj6wVgrg",
    "outputId": "03e528d2-9f0e-436a-a7a0-2ef5daccd794"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debug - Training args bf16: False\n",
      "Debug - Training args fp16: False\n",
      "Debug - GRPO config bf16: False\n",
      "Debug - GRPO config fp16: False\n",
      "Debug - Current device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/DS/lib/python3.13/site-packages/transformers/training_args.py:2029: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--hub_token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">test_run_1</strong> at: <a href='https://wandb.ai/magn3144/DeepSeekR1Training/runs/mbc0asux' target=\"_blank\">https://wandb.ai/magn3144/DeepSeekR1Training/runs/mbc0asux</a><br> View project at: <a href='https://wandb.ai/magn3144/DeepSeekR1Training' target=\"_blank\">https://wandb.ai/magn3144/DeepSeekR1Training</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250630_114807-mbc0asux/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/magn3144/Documents/GitHub/DeepSeekR1Deployment/wandb/run-20250630_115045-egkq0k6n</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/magn3144/DeepSeekR1Training/runs/egkq0k6n' target=\"_blank\">test_run_1</a></strong> to <a href='https://wandb.ai/magn3144/DeepSeekR1Training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/magn3144/DeepSeekR1Training' target=\"_blank\">https://wandb.ai/magn3144/DeepSeekR1Training</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/magn3144/DeepSeekR1Training/runs/egkq0k6n' target=\"_blank\">https://wandb.ai/magn3144/DeepSeekR1Training/runs/egkq0k6n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/DS/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/opt/miniconda3/envs/DS/lib/python3.13/site-packages/torch/utils/checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 02:04, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2 - Logging: {'step': 2, 'train_loss': 0.0261, 'learning_rate': 0.0, 'accuracy_reward': np.float64(0.375), 'format_reward': np.float64(0.0), 'reasoning_steps_reward': np.float64(nan), 'cosine_scaled_reward': np.float64(nan), 'repetition_penalty_reward': np.float64(nan), 'total_reward': np.float64(0.375)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/DS/lib/python3.13/site-packages/numpy/_core/fromnumeric.py:3860: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/miniconda3/envs/DS/lib/python3.13/site-packages/numpy/_core/_methods.py:144: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3 - Logging: {'step': 3, 'train_loss': 0.0462, 'learning_rate': 5e-05, 'accuracy_reward': np.float64(0.0), 'format_reward': np.float64(0.0), 'reasoning_steps_reward': np.float64(nan), 'cosine_scaled_reward': np.float64(nan), 'repetition_penalty_reward': np.float64(nan), 'total_reward': np.float64(0.0)}\n",
      "Step 4 - Logging: {'step': 4, 'train_loss': 0.0, 'learning_rate': 3.3333333333333335e-05, 'accuracy_reward': np.float64(0.0), 'format_reward': np.float64(0.0), 'reasoning_steps_reward': np.float64(nan), 'cosine_scaled_reward': np.float64(nan), 'repetition_penalty_reward': np.float64(nan), 'total_reward': np.float64(0.0)}\n"
     ]
    }
   ],
   "source": [
    "# Create GRPOConfig from TrainingArguments\n",
    "grpo_config = GRPOConfig(\n",
    "    **training_args.to_dict(), # Convert TrainingArguments to dictionary and unpack\n",
    "    **{\n",
    "       # REMOVED model_init_kwargs here\n",
    "       # We are passing the instantiated 'model' object, so GRPOTrainer doesn't need model_init_kwargs\n",
    "       \"num_generations\": batch_size * gradient_acc,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Debug: Print mixed precision settings\n",
    "print(f\"Debug - Training args bf16: {training_args.bf16}\")\n",
    "print(f\"Debug - Training args fp16: {training_args.fp16}\")\n",
    "print(f\"Debug - GRPO config bf16: {grpo_config.bf16}\")\n",
    "print(f\"Debug - GRPO config fp16: {grpo_config.fp16}\")\n",
    "print(f\"Debug - Current device: {device}\")\n",
    "\n",
    "train_size = 4\n",
    "test_size = 1\n",
    "train_set = dataset['train'].select(range(train_size))\n",
    "test_set = dataset['test'].select(range(test_size))\n",
    "grpo_trainer = GRPOTrainer(\n",
    "    model=model,                     # Our initialized Qwen model\n",
    "    reward_funcs=reward_functions,   # List of reward functions from previous step\n",
    "    args=grpo_config,                # GRPOConfig (created from TrainingArguments)\n",
    "    train_dataset=train_set,         # Training dataset\n",
    "    eval_dataset=test_set,           # Evaluation dataset\n",
    "    callbacks=callbacks              # List of callbacks\n",
    ")\n",
    "\n",
    "run = wandb.init(\n",
    "    project=\"DeepSeekR1Training\",\n",
    "    name=\"test_run_1\",\n",
    "    config={\n",
    "        \"learning_rate\": training_args.learning_rate,\n",
    "        \"epochs\": training_args.num_train_epochs,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Start the GRPO Training Loop\n",
    "train_result = grpo_trainer.train()\n",
    "\n",
    "# sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# api = wandb.Api()\n",
    "# runs = api.runs('magn3144/DeepSeekR1Training')\n",
    "# for run in runs:\n",
    "#     if run.name == \"test_run_1\":\n",
    "#         run.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aLbq9FzXVgri"
   },
   "outputs": [],
   "source": [
    "# Define the path to your trained model (same as OUTPUT_DIR)\n",
    "TRAINED_MODEL_PATH = \"data/Qwen-GRPO-training\"\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(TRAINED_MODEL_PATH)\n",
    "\n",
    "# Save the trained model\n",
    "grpo_trainer.save_model(TRAINED_MODEL_PATH)\n",
    "\n",
    "print(f\"GRPO Trained model saved to {TRAINED_MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tbjoV4eGVgri"
   },
   "outputs": [],
   "source": [
    "# Load the tokenizer - make sure to use trust_remote_code=True if needed\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    TRAINED_MODEL_PATH,\n",
    "    trust_remote_code=True, # If your model config requires it\n",
    "    padding_side=\"right\" # Ensure consistent padding side\n",
    ")\n",
    "\n",
    "# Set pad token if it wasn't saved or loaded correctly\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load the trained model itself\n",
    "trained_model = AutoModelForCausalLM.from_pretrained(\n",
    "    TRAINED_MODEL_PATH,\n",
    "    trust_remote_code=True, # If your model architecture requires it\n",
    "    torch_dtype=TORCH_DTYPE # Use same dtype as training for consistency\n",
    ")\n",
    "\n",
    "# Move the loaded model to your device (GPU if available)\n",
    "trained_model.to(device) # 'device' is still our CUDA device from before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3uMLRtD8Vgrj"
   },
   "outputs": [],
   "source": [
    "# Testing Inference with the Trained Model\n",
    "def test_trained_model_inference(user_input: str):\n",
    "    \"\"\"Test inference with the loaded trained model and tokenizer.\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT}, # Re-use our system prompt\n",
    "        {\"role\": \"user\", \"content\": user_input}\n",
    "    ]\n",
    "\n",
    "    # Apply chat template using our tokenizer\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Generate output using our *trained_model*\n",
    "    outputs = trained_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=200, # Maybe generate a bit longer now\n",
    "        do_sample=True,\n",
    "        temperature=0.7\n",
    "    )\n",
    "\n",
    "    # Decode the generated tokens back to text\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Test the model\n",
    "test_input = \"how are you?\"\n",
    "response = test_trained_model_inference(test_input)\n",
    "print(f\"Test Input: {test_input}\")\n",
    "print(f\"Trained Model Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a4BqqXjZVgrm"
   },
   "outputs": [],
   "source": [
    "# # Load the \"Bespoke-Stratos-17k\" dataset from bespokelabs\n",
    "# bespoke_rl = load_dataset(\"bespokelabs/Bespoke-Stratos-17k\", \"default\")\n",
    "\n",
    "# # Access the first sample in the training set\n",
    "# bespoke_rl['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wk4KWg_mVgrm"
   },
   "outputs": [],
   "source": [
    "# Model and Output Configuration (same as before, or adjust as needed)\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "OUTPUT_DIR = \"data/Qwen-SFT-training\" # New output directory for SFT model\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Training Arguments - similar to GRPO, but adjust for SFT\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,         # Adjust epochs as needed\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=2e-5,        # Adjust learning rate for SFT\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,\n",
    "    # evaluation_strategy=\"no\",\n",
    "    eval_steps=50,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,\n",
    "    save_total_limit=2,\n",
    "    dataloader_num_workers=2,\n",
    "    seed=42,\n",
    "    bf16=USE_BF16,                 # Use simplified configuration\n",
    "    fp16=USE_FP16,                 # Use simplified configuration\n",
    "    push_to_hub=False,\n",
    "    gradient_checkpointing=True,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# Model Configuration - same as before\n",
    "model_args = ModelConfig(\n",
    "    model_name_or_path=MODEL_NAME,\n",
    "    model_revision=\"main\",\n",
    "    torch_dtype=TORCH_DTYPE_STR,   # Use configuration variable\n",
    "    trust_remote_code=True,\n",
    "    attn_implementation=\"flash_attention_2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H7OiNtxCVgrn"
   },
   "outputs": [],
   "source": [
    "# Load Bespoke-Stratos-17k dataset\n",
    "dataset_sft = load_dataset(\"HuggingFaceH4/Bespoke-Stratos-17k\", split='train') # Only using train split for simplicity\n",
    "\n",
    "# Initialize tokenizer - same as before\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    trust_remote_code=True,\n",
    "    padding_side=\"right\"\n",
    ")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sft[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4NsAocMcVgrn"
   },
   "outputs": [],
   "source": [
    "# Initialize base model for SFT - same as before\n",
    "model_sft = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=TORCH_DTYPE        # Use configuration variable\n",
    ")\n",
    "\n",
    "# Initialize the SFT Trainer\n",
    "sft_trainer = SFTTrainer(\n",
    "    model=model_sft,                     # Our initialized Qwen model\n",
    "    train_dataset=dataset_sft,           # Bespoke-Stratos-17k dataset\n",
    "    processing_class=tokenizer,                 # Tokenizer\n",
    "    args=training_args,                  # Training arguments\n",
    ")\n",
    "\n",
    "# Start the SFT Training Loop\n",
    "sft_train_result = sft_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mm0IG1gsVgro"
   },
   "outputs": [],
   "source": [
    "# Saving the Trained SFT Model\n",
    "TRAINED_SFT_MODEL_PATH = \"data/Qwen-SFT-training\" # Same as OUTPUT_DIR\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(TRAINED_SFT_MODEL_PATH)\n",
    "\n",
    "# Save the trained model\n",
    "sft_trainer.save_model(TRAINED_SFT_MODEL_PATH)\n",
    "\n",
    "print(f\"SFT Trained model saved to {TRAINED_SFT_MODEL_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "DS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
